{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikiscrape\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page is loading...\n",
      "\n",
      "Search text formatted to title/proper case by default. Set second argument as 'No' to disable formatting\n",
      "Wikipedia page loaded successfully!! Type variablename.HELP() for documentation of functions.\n",
      "\n",
      "\n",
      "Armin Jozef Jacobus Daniël van Buuren OON (/væn ˈbjʊərən/; Dutch: ; born 25 December 1976) is a Dutch DJ, record producer and remixer from South Holland. Since 2001, he has hosted A State of Trance a weekly radio show, which is broadcast to nearly 40 million listeners in 84 countries on over 100 FM radio stations. According to Djs And Festivals \"the radio show propelled him to stardom and helped cultivate an interest in trance music around the world.\"\n",
      " \n",
      "\n",
      "Van Buuren has won a number of accolades. He has been ranked the number one DJ by DJ Mag a record of five times, four years in a row. He was ranked fourth on the DJ Mag Top 100 DJs list in 2015 and 2016, and third in 2017. In 2014, he was nominated for a Grammy Award for Best Dance Recording for his single \"This Is What It Feels Like\" featuring Trevor Guthrie which makes him the fourth trance artist ever to receive a Grammy Award nomination. In the United States, he holds the record for most entries, twenty-one, on the Billboard Dance/Electronic Albums chart.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "armin = wikiscrape.wiki('Armin van Buuren')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page is loading...\n",
      "\n",
      "Search text formatted to title/proper case by default. Set second argument as 'No' to disable formatting\n",
      "Wikipedia page loaded successfully!! Type variablename.HELP() for documentation of functions.\n",
      "\n",
      "\n",
      "Tijs Michiel Verwest OON (Dutch: ; born 17 January 1969), better known by his stage name Tiësto (/tiˈɛstoʊ/ tee-ES-toh; Dutch: ), is a Dutch DJ and record producer from Breda. He was named \"the Greatest DJ of All Time\" by Mix magazine in a poll voted by the fans. In 2013, he was voted by DJ Mag readers as the \"best DJ of the last 20 years\". He is also regarded as the \"Godfather of EDM\" by many sources.\n",
      " \n",
      "\n",
      "In 1997, he founded the label Black Hole Recordings with Arny Bink, where he released the Magik and In Search of Sunrise CD series. Tiësto met producer Dennis Waakop Reijers in 1998; the two have worked together extensively since then.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tiesto = wikiscrape.wiki('Tiesto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page is loading...\n",
      "\n",
      "Search text formatted to title/proper case by default. Set second argument as 'No' to disable formatting\n",
      "Wikipedia page loaded successfully!! Type variablename.HELP() for documentation of functions.\n",
      "\n",
      "\n",
      "Matthias Paul (German: ; born 16 December 1971), known professionally as Paul van Dyk (/daɪk/; German: ) is a German DJ, record producer and musician. One of the first true renowned DJs, van Dyk was the first artist to receive a Grammy Award nomination in the newly added category of Best Dance/Electronic album for his 2003 release Reflections. He was named the World's number one DJ in both 2005 and 2006, something only few DJs have ever achieved. He was the first ever DJ to be named number one by Mixmag in 2005. By 2008, he had sold over 3 million albums worldwide.\n",
      " \n",
      "\n",
      "A trance producer starting in the early 1990s, van Dyk quickly achieved popularity with his remix of \"Love Stimulation\" by Humate on the record label MFS in 1993 and with his hit single \"For an Angel\" but, in recent times, he no longer likes to describe his music as trance, but rather simply as electronic music.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paulvandyk = wikiscrape.wiki('Paul van Dyk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page is loading...\n",
      "\n",
      "Search text formatted to title/proper case by default. Set second argument as 'No' to disable formatting\n",
      "Wikipedia page loaded successfully!! Type variablename.HELP() for documentation of functions.\n",
      "\n",
      "\n",
      "Barack Hussein Obama II (/bəˈrɑːk huːˈseɪn oʊˈbɑːmə/ (listen); born August 4, 1961) is an American attorney and politician who served as the 44th president of the United States from 2009 to 2017. A member of the Democratic Party he was the first African American to be elected to the presidency. He previously served as a U.S. senator from Illinois from 2005 to 2008.\n",
      " \n",
      "\n",
      "Obama was born in Honolulu Hawaii. After graduating from Columbia University in 1983, he worked as a community organizer in Chicago. In 1988, he enrolled in Harvard Law School where he was the first black president of the Harvard Law Review. After graduating, he became a civil rights attorney and an academic, teaching constitutional law at the University of Chicago Law School from 1992 to 2004. He represented the 13th district for three terms in the Illinois Senate from 1997 to 2004, when he ran for the U.S. Senate. He received national attention in 2004 with his March primary win, his well-received July Democratic National Convention keynote address and his landslide November election to the Senate. In 2008, he was nominated for president a year after his campaign began and after a close primary campaign against Hillary Clinton. He was elected over Republican John McCain and was inaugurated on January 20, 2009. Nine months later, he was named the 2009 Nobel Peace Prize laureate.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "obama = wikiscrape.wiki('Obama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page is loading...\n",
      "\n",
      "Search text formatted to title/proper case by default. Set second argument as 'No' to disable formatting\n",
      "Wikipedia page loaded successfully!! Type variablename.HELP() for documentation of functions.\n",
      "\n",
      "\n",
      "Donald John Trump (born June 14, 1946) is the 45th and current president of the United States. Before entering politics, he was a businessman and television personality.\n",
      " \n",
      "\n",
      "Trump was born and raised in the New York City borough of Queens and received an economics degree from the Wharton School. He was appointed president of his family's real estate business in 1971, renamed it The Trump Organization and expanded it from Queens and Brooklyn into Manhattan. The company built or renovated skyscrapers, hotels, casinos, and golf courses. Trump later started various side ventures, including licensing his name for real estate and consumer products. He managed the company until his 2017 inauguration. He co-authored several books including The Art of the Deal. He owned the Miss Universe and Miss USA beauty pageants from 1996 to 2015, and he produced and hosted The Apprentice a reality television show, from 2003 to 2015. Forbes estimates his net worth to be $3.1 billion.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trump = wikiscrape.wiki('Donald Trump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page is loading...\n",
      "\n",
      "Search text formatted to title/proper case by default. Set second argument as 'No' to disable formatting\n",
      "Wikipedia page loaded successfully!! Type variablename.HELP() for documentation of functions.\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "Bernard Sanders (born September 8, 1941) is an American politician who has served as the junior United States Senator from Vermont since 2007. The longest-serving Independent in congressional history, he was elected to the U.S. House of Representatives in 1990 and caucuses with the Democratic Party enabling his appointment to congressional committees and at times giving Democrats a majority.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bernie = wikiscrape.wiki('Bernie Sanders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The wiki() class accepts 4 arguments. The first one is a compulsory title of the Wikipedia page. Second is to preserve the search casing (Yes/No).\n",
      "\n",
      "Third is for language settings (e.g. English, de, francais, etc.). Fourth is for implementing NLTK stoplist in provided languages (Yes/No).\n",
      "\n",
      "commonwords accepts 1 optional argument (default: 100) for the number of most common words in the site to show\n",
      "\n",
      "commonwordspct accepts 1 optional argument (default: 10) on the percentage threshold of word count to determine the most common words\n",
      "\n",
      "plotwords accepts 1 optional argument (default: 20) for the number of most common words to show as a GRAPH\n",
      "\n",
      "totalwords accepts 0 arguments and shows the total word count and unique word count\n",
      "\n",
      "summary accepts 1 optional argument for the number of paragraphs (default: 2) and gives a summary of the Wikipedia page\n",
      "\n",
      "gettext accepts 0 arguments and retrieves the full text of the Wikipedia title\n",
      "\n"
     ]
    }
   ],
   "source": [
    "armin.HELP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jx\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "['van.n.01', 'buuren.n.01', 'armin.n.01', 'trance.n.01', 'music.n.01', 'state.n.01', '2010.n.01', 'dj.n.01', 'album.n.01', 'radio.n.01', 'dutch.n.01', '2015.n.01', 'released.n.01', 'single.n.01', 'original.n.01', '2016-06-21.n.01', 'armada.n.01', '2012.n.01', 'show.n.01', 'mag.n.01', '2017.n.01', '2013.n.01', 'wikipedia.n.01', '2014.n.01', 'netherlands.n.01', '2016.n.01', 'studio.n.01', '10.n.01', 'number.n.01', 'episode.n.01', '1.n.01', 'artists.n.01', 'djs.n.01', 'award.n.01', 'announced.n.01', '17.n.01', '2008.n.01', '2009.n.01', '2011.n.01', 'track.n.01']\n",
      "['van.n.01', 'dyk.n.01', 'paul.n.01', 'album.n.01', '2009.n.01', 'music.n.01', 'armin.n.01', 'buuren.n.01', '2016.n.01', '2016-06-07.n.01', 'wikipedia.n.01', 'dj.n.01', 'german.n.01', 'berlin.n.01', 'politics.n.01', 'released.n.01', 'dancing.n.01', '2005.n.01', 'single.n.01', '2.n.01', 'identifiers.n.01', '2003.n.01', '2008.n.01', 'dance.n.01', 'between.n.01', '/.n.01', 'djs.n.01', 'trance.n.01', 'radio.n.01', 'records.n.01', 'original.n.01', '2007.n.01', '2012.n.01', '28.n.01', 'ultra.n.01', 'mfs.n.01', 'ways.n.01', 'artists.n.01', 'dyks.n.01', 'song.n.01']\n",
      "Related index is 88%\n"
     ]
    }
   ],
   "source": [
    "#IF YOU FORGOT TO IMPORT PACKAGES, THE TRY PART WONT RUN, ONLY EXCEPT AND PERCENTAGE WILL BE ERRORNEOUS\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "wn = nltk.corpus.wordnet #the corpus reader\n",
    "import math\n",
    "\n",
    "dotn01 = ('.','n','.','0','1')\n",
    "arminlist = []\n",
    "for key in armin.commonwords(40):\n",
    "    arminslice = list(key)\n",
    "    for letter in dotn01:\n",
    "        arminslice.append(letter)\n",
    "    arminslice = ''.join(arminslice)\n",
    "    arminlist.append(arminslice)\n",
    "\n",
    "print(arminlist)\n",
    "\n",
    "paulvandyklist = []\n",
    "for key in paulvandyk.commonwords(40):\n",
    "    paulvandykslice = list(key)\n",
    "    for letter in dotn01:\n",
    "        paulvandykslice.append(letter)\n",
    "    paulvandykslice = ''.join(paulvandykslice)\n",
    "    paulvandyklist.append(paulvandykslice)\n",
    "\n",
    "print(paulvandyklist)\n",
    "    \n",
    "count = 0\n",
    "sum = 0\n",
    "\n",
    "for word1 in arminlist:\n",
    "    for word2 in paulvandyklist:\n",
    "        try:\n",
    "            x = None\n",
    "            y = None\n",
    "            x = wn.synset(word1)\n",
    "            y = wn.synset(word2)\n",
    "            sum += x.path_similarity(y) * math.exp(2.5 * x.path_similarity(y)) + 14 * math.log(0.92+x.path_similarity(y))\n",
    "            count += 1\n",
    "        except:\n",
    "            if word1 == word2 and re.findall(r\"\\d+.n.01\", word1) == []: #remove years being counted as match yyyy.n.01\n",
    "                sum += 21.32\n",
    "                count += 1\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "if count != 0:\n",
    "    percent = round(sum/count*100)\n",
    "    print('Related index is ' + str(percent) + '%')\n",
    "else:\n",
    "    print('No relation index can be calculated as words are all foreign')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['van.n.01', 'buuren.n.01', 'armin.n.01', 'trance.n.01', 'music.n.01', 'state.n.01', '2010.n.01', 'dj.n.01', 'album.n.01', 'radio.n.01', 'dutch.n.01', '2015.n.01', 'released.n.01', 'single.n.01', 'original.n.01', '2016-06-21.n.01', 'armada.n.01', '2012.n.01', 'show.n.01', 'mag.n.01', '2017.n.01', '2013.n.01', 'wikipedia.n.01', '2014.n.01', 'netherlands.n.01', '2016.n.01', 'studio.n.01', '10.n.01', 'number.n.01', 'episode.n.01', '1.n.01', 'artists.n.01', 'djs.n.01', 'award.n.01', 'announced.n.01', '17.n.01', '2008.n.01', '2009.n.01', '2011.n.01', 'track.n.01']\n",
      "['tiësto.n.01', 'album.n.01', 'released.n.01', 'dj.n.01', '2016.n.01', '2008.n.01', 'original.n.01', 'life.n.01', '2017.n.01', '2.n.01', '2011.n.01', 'tour.n.01', '2009.n.01', 'music.n.01', '23.n.01', 'search.n.01', 'sunrise.n.01', 'live.n.01', 'dutch.n.01', 'trance.n.01', '6.n.01', 'world.n.01', '2004.n.01', 'van.n.01', 'tiesto.n.01', 'club.n.01', 'billboard.n.01', 'concert.n.01', '2015.n.01', 'elements.n.01', '2014.n.01', '5.n.01', 'wikipedia.n.01', 'magazine.n.01', 'remixed.n.01', 'armin.n.01', 'buuren.n.01', '28.n.01', 'gouryella.n.01', 'called.n.01']\n",
      "Related index is 68%\n"
     ]
    }
   ],
   "source": [
    "#import nltk\n",
    "#nltk.download('wordnet')\n",
    "#from nltk.corpus import wordnet\n",
    "#wn = nltk.corpus.wordnet #the corpus reader\n",
    "\n",
    "dotn01 = ('.','n','.','0','1')\n",
    "arminlist = []\n",
    "for key in armin.commonwords(40):\n",
    "    arminslice = list(key)\n",
    "    for letter in dotn01:\n",
    "        arminslice.append(letter)\n",
    "    arminslice = ''.join(arminslice)\n",
    "    arminlist.append(arminslice)\n",
    "\n",
    "print(arminlist)\n",
    "\n",
    "tiestolist = []\n",
    "for key in tiesto.commonwords(40):\n",
    "    tiestoslice = list(key)\n",
    "    for letter in dotn01:\n",
    "        tiestoslice.append(letter)\n",
    "    tiestoslice = ''.join(tiestoslice)\n",
    "    tiestolist.append(tiestoslice)\n",
    "\n",
    "print(tiestolist)\n",
    "    \n",
    "count = 0\n",
    "sum = 0\n",
    "\n",
    "for word1 in arminlist:\n",
    "    for word2 in tiestolist:\n",
    "        try:\n",
    "            x = None\n",
    "            y = None\n",
    "            x = wn.synset(word1)\n",
    "            y = wn.synset(word2)\n",
    "            sum += x.path_similarity(y) * math.exp(2.5 * x.path_similarity(y)) + 14 * math.log(0.92+x.path_similarity(y))\n",
    "            count += 1\n",
    "        except:\n",
    "            if word1 == word2 and re.findall(r\"\\d+.n.01\", word1) == []: #remove years being counted as match yyyy.n.01\n",
    "                sum += 21.32\n",
    "                count += 1\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "if count != 0:\n",
    "    percent = round(sum/count*100)\n",
    "    print('Related index is ' + str(percent) + '%')\n",
    "else:\n",
    "    print('No relation index can be calculated as words are all foreign')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page is loading...\n",
      "\n",
      "Search text formatted to title/proper case by default. Set second argument as 'No' to disable formatting\n",
      "Wikipedia page loaded successfully!! Type variablename.HELP() for documentation of functions.\n",
      "\n",
      "\n",
      "Martijn Gerard Garritsen (Dutch: ), professionally known as Martin Garrix is a Dutch DJ and record producer from Amstelveen.  His most known singles are \"Animals\", \"In the Name of Love\", and \"Scared to be Lonely\". He was ranked number one on DJ Mag's Top 100 DJs list for three consecutive years (2016, 2017, and 2018).\n",
      " \n",
      "\n",
      "He has performed at music festivals such as Coachella Electric Daisy Carnival Ultra Music Festival Tomorrowland and Creamfields. He is known as the youngest DJ to headline 2014 Ultra Music Festival at the age of 17. He was a resident DJ at Spain's Hï Ibiza (2017) and Ushuaïa Ibiza (2016 and 2018). He founded the label Stmpd Rcrds in 2016, months after leaving Spinnin' Records and before signing with Sony Music.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "martin = wikiscrape.wiki('martin garrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['garrix.n.01', 'martin.n.01', '2017.n.01', '2016.n.01', 'dj.n.01', 'released.n.01', 'music.n.01', '2018.n.01', 'billboard.n.01', '2015.n.01', 'single.n.01', '1.n.01', 'records.n.01', '&amp.n.01', 'edm.n.01', 'dutch.n.01', 'track.n.01', 'collaboration.n.01', 'song.n.01', '20.n.01', 'top.n.01', 'djs.n.01', 'festival.n.01', 'announced.n.01', 'video.n.01', 'mag.n.01', 'spinnin.n.01', 'release.n.01', 'ultra.n.01', '2013.n.01', 'van.n.01', 'artists.n.01', '2019.n.01', 'watch.n.01', 'wikipedia.n.01', '2014.n.01', '16.n.01', '10.n.01', 'featuring.n.01', '100.n.01']\n",
      "['tiësto.n.01', 'album.n.01', 'released.n.01', 'dj.n.01', '2016.n.01', '2008.n.01', 'original.n.01', 'life.n.01', '2017.n.01', '2.n.01', '2011.n.01', 'tour.n.01', '2009.n.01', 'music.n.01', '23.n.01', 'search.n.01', 'sunrise.n.01', 'live.n.01', 'dutch.n.01', 'trance.n.01', '6.n.01', 'world.n.01', '2004.n.01', 'van.n.01', 'tiesto.n.01', 'club.n.01', 'billboard.n.01', 'concert.n.01', '2015.n.01', 'elements.n.01', '2014.n.01', '5.n.01', 'wikipedia.n.01', 'magazine.n.01', 'remixed.n.01', 'armin.n.01', 'buuren.n.01', '28.n.01', 'gouryella.n.01', 'called.n.01']\n",
      "Related index is 64%\n"
     ]
    }
   ],
   "source": [
    "#import nltk\n",
    "#nltk.download('wordnet')\n",
    "#from nltk.corpus import wordnet\n",
    "#wn = nltk.corpus.wordnet #the corpus reader\n",
    "\n",
    "dotn01 = ('.','n','.','0','1')\n",
    "martinlist = []\n",
    "for key in martin.commonwords(40):\n",
    "    martinslice = list(key)\n",
    "    for letter in dotn01:\n",
    "        martinslice.append(letter)\n",
    "    martinslice = ''.join(martinslice)\n",
    "    martinlist.append(martinslice)\n",
    "\n",
    "print(martinlist)\n",
    "\n",
    "tiestolist = []\n",
    "for key in tiesto.commonwords(40):\n",
    "    tiestoslice = list(key)\n",
    "    for letter in dotn01:\n",
    "        tiestoslice.append(letter)\n",
    "    tiestoslice = ''.join(tiestoslice)\n",
    "    tiestolist.append(tiestoslice)\n",
    "\n",
    "print(tiestolist)\n",
    "    \n",
    "count = 0\n",
    "sum = 0\n",
    "\n",
    "for word1 in martinlist:\n",
    "    for word2 in tiestolist:\n",
    "        try:\n",
    "            x = None\n",
    "            y = None\n",
    "            x = wn.synset(word1)\n",
    "            y = wn.synset(word2)\n",
    "            sum += x.path_similarity(y) * math.exp(2.5 * x.path_similarity(y)) + 14 * math.log(0.92+x.path_similarity(y))\n",
    "            count += 1\n",
    "        except:\n",
    "            if word1 == word2 and re.findall(r\"\\d+.n.01\", word1) == []: #remove years being counted as match yyyy.n.01\n",
    "                sum += 21.32\n",
    "                count += 1\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "if count != 0:\n",
    "    percent = round(sum/count*100)\n",
    "    print('Related index is ' + str(percent) + '%')\n",
    "else:\n",
    "    print('No relation index can be calculated as words are all foreign')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['van.n.01', 'buuren.n.01', 'armin.n.01', 'trance.n.01', 'music.n.01', 'state.n.01', '2010.n.01', 'dj.n.01', 'album.n.01', 'radio.n.01', 'dutch.n.01', '2015.n.01', 'released.n.01', 'single.n.01', 'original.n.01', '2016-06-21.n.01', 'armada.n.01', '2012.n.01', 'show.n.01', 'mag.n.01', '2017.n.01', '2013.n.01', 'wikipedia.n.01', '2014.n.01', 'netherlands.n.01', '2016.n.01', 'studio.n.01', '10.n.01', 'number.n.01', 'episode.n.01', '1.n.01', 'artists.n.01', 'djs.n.01', 'award.n.01', 'announced.n.01', '17.n.01', '2008.n.01', '2009.n.01', '2011.n.01', 'track.n.01']\n",
      "['sanders.n.01', 'bernie.n.01', '2015.n.01', '2016.n.01', 'democratic.n.01', '2018.n.01', 'party.n.01', '2017.n.01', 'campaign.n.01', 'vermont.n.01', 'states.n.01', 'united.n.01', 'senate.n.01', 'american.n.01', 'socialist.n.01', 'act.n.01', 'election.n.01', 'times.n.01', 'v.n.01', 'political.n.01', 'clinton.n.01', 'york.n.01', 'against.n.01', 'sanderss.n.01', '19.n.01', '4.n.01', 'washington.n.01', 'bill.n.01', 'house.n.01', 'said.n.01', 'post.n.01', '18.n.01', 'vote.n.01', 'committee.n.01', 'presidential.n.01', 'rights.n.01', 'war.n.01', 'socialism.n.01', 'independent.n.01', 'during.n.01']\n",
      "Related index is 18%\n"
     ]
    }
   ],
   "source": [
    "#import nltk\n",
    "#nltk.download('wordnet')\n",
    "#from nltk.corpus import wordnet\n",
    "#wn = nltk.corpus.wordnet #the corpus reader\n",
    "\n",
    "dotn01 = ('.','n','.','0','1')\n",
    "arminlist = []\n",
    "for key in armin.commonwords(40):\n",
    "    arminslice = list(key)\n",
    "    for letter in dotn01:\n",
    "        arminslice.append(letter)\n",
    "    arminslice = ''.join(arminslice)\n",
    "    arminlist.append(arminslice)\n",
    "\n",
    "print(arminlist)\n",
    "\n",
    "bernielist = []\n",
    "for key in bernie.commonwords(40):\n",
    "    bernieslice = list(key)\n",
    "    for letter in dotn01:\n",
    "        bernieslice.append(letter)\n",
    "    bernieslice = ''.join(bernieslice)\n",
    "    bernielist.append(bernieslice)\n",
    "\n",
    "print(bernielist)\n",
    "    \n",
    "count = 0\n",
    "sum = 0\n",
    "\n",
    "for word1 in arminlist:\n",
    "    for word2 in bernielist:\n",
    "        try:\n",
    "            x = None\n",
    "            y = None\n",
    "            x = wn.synset(word1)\n",
    "            y = wn.synset(word2)\n",
    "            sum += x.path_similarity(y) * math.exp(2.5 * x.path_similarity(y)) + 14 * math.log(0.92+x.path_similarity(y))\n",
    "            count += 1\n",
    "        except:\n",
    "            if word1 == word2 and re.findall(r\"\\d+.n.01\", word1) == []: #remove years being counted as match yyyy.n.01:\n",
    "                sum += 21.32\n",
    "                count += 1\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "if count != 0:\n",
    "    percent = round(sum/count*100)\n",
    "    print('Related index is ' + str(percent) + '%')\n",
    "else:\n",
    "    print('No relation index can be calculated as words are all foreign')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['trump.n.01', '2018.n.01', '2017.n.01', '2016.n.01', 'donald.n.01', 'trumps.n.01', 'york.n.01', 'times.n.01', '2019.n.01', '2015.n.01', 'president.n.01', 'news.n.01', 'washington.n.01', 'post.n.01', 'had.n.01', 'campaign.n.01', 'presidential.n.01', 'states.n.01', '12.n.01', '3.n.01', 'cnn.n.01', 'michael.n.01', '16.n.01', 'united.n.01', 'election.n.01', '14.n.01', '18.n.01', '20.n.01', 'house.n.01', 'during.n.01', 'said.n.01', '22.n.01', 'been.n.01', '15.n.01', '21.n.01', 'white.n.01', '6.n.01', 'were.n.01', '7.n.01', 'tax.n.01']\n",
      "['sanders.n.01', 'bernie.n.01', '2015.n.01', '2016.n.01', 'democratic.n.01', '2018.n.01', 'party.n.01', '2017.n.01', 'campaign.n.01', 'vermont.n.01', 'states.n.01', 'united.n.01', 'senate.n.01', 'american.n.01', 'socialist.n.01', 'act.n.01', 'election.n.01', 'times.n.01', 'v.n.01', 'political.n.01', 'clinton.n.01', 'york.n.01', 'against.n.01', 'sanderss.n.01', '19.n.01', '4.n.01', 'washington.n.01', 'bill.n.01', 'house.n.01', 'said.n.01', 'post.n.01', '18.n.01', 'vote.n.01', 'committee.n.01', 'presidential.n.01', 'rights.n.01', 'war.n.01', 'socialism.n.01', 'independent.n.01', 'during.n.01']\n",
      "Related index is 71%\n"
     ]
    }
   ],
   "source": [
    "#import nltk\n",
    "#nltk.download('wordnet')\n",
    "#from nltk.corpus import wordnet\n",
    "#wn = nltk.corpus.wordnet #the corpus reader\n",
    "\n",
    "dotn01 = ('.','n','.','0','1')\n",
    "trumplist = []\n",
    "for key in trump.commonwords(40):\n",
    "    trumpslice = list(key)\n",
    "    for letter in dotn01:\n",
    "        trumpslice.append(letter)\n",
    "    trumpslice = ''.join(trumpslice)\n",
    "    trumplist.append(trumpslice)\n",
    "\n",
    "print(trumplist)\n",
    "\n",
    "bernielist = []\n",
    "for key in bernie.commonwords(40):\n",
    "    bernieslice = list(key)\n",
    "    for letter in dotn01:\n",
    "        bernieslice.append(letter)\n",
    "    bernieslice = ''.join(bernieslice)\n",
    "    bernielist.append(bernieslice)\n",
    "\n",
    "print(bernielist)\n",
    "    \n",
    "count = 0\n",
    "sum = 0\n",
    "\n",
    "for word1 in trumplist:\n",
    "    for word2 in bernielist:\n",
    "        try:\n",
    "            x = None\n",
    "            y = None\n",
    "            x = wn.synset(word1)\n",
    "            y = wn.synset(word2)\n",
    "            sum += x.path_similarity(y) * math.exp(2.5 * x.path_similarity(y)) + 14 * math.log(0.92+x.path_similarity(y))\n",
    "            count += 1\n",
    "        except:\n",
    "            if word1 == word2 and re.findall(r\"\\d+.n.01\", word1) == []: #remove years being counted as match yyyy.n.01\n",
    "                sum += 21.32\n",
    "                count += 1\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "if count != 0:\n",
    "    percent = round(sum/count*100)\n",
    "    print('Related index is ' + str(percent) + '%')\n",
    "else:\n",
    "    print('No relation index can be calculated as words are all foreign')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['obama.n.01', '2008.n.01', '2009.n.01', 'original.n.01', '2010.n.01', '2011.n.01', 'barack.n.01', '2007.n.01', '2012.n.01', 'obamas.n.01', 'chicago.n.01', 'york.n.01', 'president.n.01', 'times.n.01', '2017.n.01', '2015.n.01', 'pp.n.01', 'campaign.n.01', '3.n.01', '2004.n.01', 'united.n.01', '/.n.01', '2016.n.01', '2014.n.01', 'act.n.01', '15.n.01', '20.n.01', '5.n.01', 'senate.n.01', '1.n.01', 'law.n.01', '2013.n.01', 'states.n.01', '18.n.01', 'washington.n.01', '9.n.01', '27.n.01', 'house.n.01', '7.n.01', '24.n.01']\n",
      "['sanders.n.01', 'bernie.n.01', '2015.n.01', '2016.n.01', 'democratic.n.01', '2018.n.01', 'party.n.01', '2017.n.01', 'campaign.n.01', 'vermont.n.01', 'states.n.01', 'united.n.01', 'senate.n.01', 'american.n.01', 'socialist.n.01', 'act.n.01', 'election.n.01', 'times.n.01', 'v.n.01', 'political.n.01', 'clinton.n.01', 'york.n.01', 'against.n.01', 'sanderss.n.01', '19.n.01', '4.n.01', 'washington.n.01', 'bill.n.01', 'house.n.01', 'said.n.01', 'post.n.01', '18.n.01', 'vote.n.01', 'committee.n.01', 'presidential.n.01', 'rights.n.01', 'war.n.01', 'socialism.n.01', 'independent.n.01', 'during.n.01']\n",
      "Related index is 66%\n"
     ]
    }
   ],
   "source": [
    "#import nltk\n",
    "#nltk.download('wordnet')\n",
    "#from nltk.corpus import wordnet\n",
    "#wn = nltk.corpus.wordnet #the corpus reader\n",
    "\n",
    "dotn01 = ('.','n','.','0','1')\n",
    "obamalist = []\n",
    "for key in obama.commonwords(40):\n",
    "    obamaslice = list(key)\n",
    "    for letter in dotn01:\n",
    "        obamaslice.append(letter)\n",
    "    obamaslice = ''.join(obamaslice)\n",
    "    obamalist.append(obamaslice)\n",
    "\n",
    "print(obamalist)\n",
    "\n",
    "bernielist = []\n",
    "for key in bernie.commonwords(40):\n",
    "    bernieslice = list(key)\n",
    "    for letter in dotn01:\n",
    "        bernieslice.append(letter)\n",
    "    bernieslice = ''.join(bernieslice)\n",
    "    bernielist.append(bernieslice)\n",
    "\n",
    "print(bernielist)\n",
    "    \n",
    "count = 0\n",
    "sum = 0\n",
    "\n",
    "for word1 in obamalist:\n",
    "    for word2 in bernielist:\n",
    "        try:\n",
    "            x = None\n",
    "            y = None\n",
    "            x = wn.synset(word1)\n",
    "            y = wn.synset(word2)\n",
    "            sum += x.path_similarity(y) * math.exp(2.5 * x.path_similarity(y)) + 14 * math.log(0.92+x.path_similarity(y))\n",
    "            count += 1\n",
    "        except:\n",
    "            if word1 == word2 and re.findall(r\"\\d+.n.01\", word1) == []: #remove years being counted as match yyyy.n.01\n",
    "                sum += 21.32\n",
    "                count += 1\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "if count != 0:\n",
    "    percent = round(sum/count*100)\n",
    "    print('Related index is ' + str(percent) + '%')\n",
    "else:\n",
    "    print('No relation index can be calculated as words are all foreign')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['obama.n.01', '2008.n.01', '2009.n.01', 'original.n.01', '2010.n.01', '2011.n.01', 'barack.n.01', '2007.n.01', '2012.n.01', 'obamas.n.01', 'chicago.n.01', 'york.n.01', 'president.n.01', 'times.n.01', '2017.n.01', '2015.n.01', 'pp.n.01', 'campaign.n.01', '3.n.01', '2004.n.01', 'united.n.01', '/.n.01', '2016.n.01', '2014.n.01', 'act.n.01', '15.n.01', '20.n.01', '5.n.01', 'senate.n.01', '1.n.01', 'law.n.01', '2013.n.01', 'states.n.01', '18.n.01', 'washington.n.01', '9.n.01', '27.n.01', 'house.n.01', '7.n.01', '24.n.01']\n",
      "['tiësto.n.01', 'album.n.01', 'released.n.01', 'dj.n.01', '2016.n.01', '2008.n.01', 'original.n.01', 'life.n.01', '2017.n.01', '2.n.01', '2011.n.01', 'tour.n.01', '2009.n.01', 'music.n.01', '23.n.01', 'search.n.01', 'sunrise.n.01', 'live.n.01', 'dutch.n.01', 'trance.n.01', '6.n.01', 'world.n.01', '2004.n.01', 'van.n.01', 'tiesto.n.01', 'club.n.01', 'billboard.n.01', 'concert.n.01', '2015.n.01', 'elements.n.01', '2014.n.01', '5.n.01', 'wikipedia.n.01', 'magazine.n.01', 'remixed.n.01', 'armin.n.01', 'buuren.n.01', '28.n.01', 'gouryella.n.01', 'called.n.01']\n",
      "Related index is 45%\n"
     ]
    }
   ],
   "source": [
    "#import nltk\n",
    "#nltk.download('wordnet')\n",
    "#from nltk.corpus import wordnet\n",
    "#wn = nltk.corpus.wordnet #the corpus reader\n",
    "\n",
    "dotn01 = ('.','n','.','0','1')\n",
    "obamalist = []\n",
    "for key in obama.commonwords(40):\n",
    "    obamaslice = list(key)\n",
    "    for letter in dotn01:\n",
    "        obamaslice.append(letter)\n",
    "    obamaslice = ''.join(obamaslice)\n",
    "    obamalist.append(obamaslice)\n",
    "\n",
    "print(obamalist)\n",
    "\n",
    "tiestolist = []\n",
    "for key in tiesto.commonwords(40):\n",
    "    tiestoslice = list(key)\n",
    "    for letter in dotn01:\n",
    "        tiestoslice.append(letter)\n",
    "    tiestoslice = ''.join(tiestoslice)\n",
    "    tiestolist.append(tiestoslice)\n",
    "\n",
    "print(tiestolist)\n",
    "    \n",
    "count = 0\n",
    "sum = 0\n",
    "\n",
    "for word1 in obamalist:\n",
    "    for word2 in tiestolist:\n",
    "        try:\n",
    "            x = wn.synset(word1)\n",
    "            y = wn.synset(word2)\n",
    "            sum += x.path_similarity(y) * math.exp(2.5 * x.path_similarity(y)) + 14 * math.log(0.92+x.path_similarity(y))\n",
    "            count += 1\n",
    "        except:\n",
    "            if word1 == word2 and len(word1) != 9 and re.findall(r\"\\d+.n.01\", word1) == []: #remove years being counted as match yyyy.n.01\n",
    "                sum += 21.32\n",
    "                count += 1\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "if count != 0:\n",
    "    percent = round(sum/count*100)\n",
    "    print('Related index is ' + str(percent) + '%')\n",
    "else:\n",
    "    print('No relation index can be calculated as words are all foreign')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page is loading...\n",
      "\n",
      "Search text formatted to title/proper case by default. Set second argument as 'No' to disable formatting\n",
      "Wikipedia page loaded successfully!! Type variablename.HELP() for documentation of functions.\n",
      "\n",
      "\n",
      "Nickelback is a Canadian rock band formed in 1995 in Hanna, Alberta Canada. The band is composed of guitarist and lead vocalist Chad Kroeger guitarist, keyboardist and backing vocalist Ryan Peake bassist Mike Kroeger, and drummer Daniel Adair. The band went through several drummer changes between 1995 and 2005, achieving its current lineup when Adair replaced drummer Ryan Vikedal.\n",
      " \n",
      "\n",
      "Nickelback is one of the most commercially successful Canadian rock bands, having sold more than 50 million albums worldwide. In 2009, Billboard ranked them the most successful rock group of that decade; their song \"How You Remind Me\" was listed as the best-selling rock song and the fourth best-selling song of the decade overall. They were listed number seven on the Billboard top artist of the decade, with four albums listed on the Billboard top albums of the decade.\n",
      "\n",
      "Page is loading...\n",
      "\n",
      "Search text formatted to title/proper case by default. Set second argument as 'No' to disable formatting\n",
      "Wikipedia page loaded successfully!! Type variablename.HELP() for documentation of functions.\n",
      "\n",
      "\n",
      "Green Day is an American rock band formed in 1986 by lead vocalist and guitarist Billie Joe Armstrong and bassist Mike Dirnt. For much of the band's career, they have been a trio with drummer Tré Cool who replaced John Kiffmeyer in 1990 prior to the recording of the band's second studio album, Kerplunk (1991). \n",
      " \n",
      "\n",
      "Green Day was originally part of the punk scene at the DIY 924 Gilman Street club in Berkeley, California. The band's early releases were with the independent record label Lookout! Records. In 1994, their major label debut Dookie released through Reprise Records became a breakout success and eventually shipped over 10 million copies in the U.S. Green Day is credited alongside fellow California punk bands including Sublime Bad Religion The Offspring and Rancid with popularizing mainstream interest in punk rock in the United States.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nickelback = wikiscrape.wiki('nickelback')\n",
    "twentyone = wikiscrape.wiki('green day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nickelback.n.01', 'album.n.01', 'band.n.01', '2012.n.01', 'original.n.01', 'rock.n.01', '2011.n.01', '2010.n.01', 'released.n.01', 'kroeger.n.01', 'billboard.n.01', '2008.n.01', 'single.n.01', 'tour.n.01', '5.n.01', 'number.n.01', 'music.n.01', 'chad.n.01', 'dark.n.01', 'top.n.01', 'horse.n.01', '3.n.01', 'song.n.01', '2015.n.01', 'canadian.n.01', '2009.n.01', '28.n.01', 'wikipedia.n.01', 'bands.n.01', 'albums.n.01', 'roadrunner.n.01', 'records.n.01', 'release.n.01', 'singles.n.01', 'here.n.01', 'award.n.01', 'socan.n.01', '2013.n.01', 'peaked.n.01', '1.n.01']\n",
      "['green.n.01', 'original.n.01', 'band.n.01', 'album.n.01', 'rock.n.01', '2009.n.01', '2010.n.01', 'idiot.n.01', 'american.n.01', '2012.n.01', '2016.n.01', '2015.n.01', 'punk.n.01', 'armstrong.n.01', '2013.n.01', 'days.n.01', 'released.n.01', 'music.n.01', 'bands.n.01', '2011.n.01', 'were.n.01', 'joe.n.01', '22.n.01', 'albums.n.01', '2018.n.01', 'billie.n.01', 'best.n.01', 'awards.n.01', 'musical.n.01', 'broadway.n.01', 'songs.n.01', 'tour.n.01', '2014.n.01', '10.n.01', 'dirnt.n.01', 'group.n.01', 'song.n.01', 'rolling.n.01', '11.n.01', 'records.n.01']\n",
      "Related index is 71%\n"
     ]
    }
   ],
   "source": [
    "#import nltk\n",
    "#nltk.download('wordnet')\n",
    "#from nltk.corpus import wordnet\n",
    "#wn = nltk.corpus.wordnet #the corpus reader\n",
    "\n",
    "dotn01 = ('.','n','.','0','1')\n",
    "nickelbacklist = []\n",
    "for key in nickelback.commonwords(40):\n",
    "    nickelbackslice = list(key)\n",
    "    for letter in dotn01:\n",
    "        nickelbackslice.append(letter)\n",
    "    nickelbackslice = ''.join(nickelbackslice)\n",
    "    nickelbacklist.append(nickelbackslice)\n",
    "\n",
    "print(nickelbacklist)\n",
    "\n",
    "twentyonelist = []\n",
    "for key in twentyone.commonwords(40):\n",
    "    twentyoneslice = list(key)\n",
    "    for letter in dotn01:\n",
    "        twentyoneslice.append(letter)\n",
    "    twentyoneslice = ''.join(twentyoneslice)\n",
    "    twentyonelist.append(twentyoneslice)\n",
    "\n",
    "print(twentyonelist)\n",
    "    \n",
    "count = 0\n",
    "sum = 0\n",
    "\n",
    "for word1 in nickelbacklist:\n",
    "    for word2 in twentyonelist:\n",
    "        try:\n",
    "            x = wn.synset(word1)\n",
    "            y = wn.synset(word2)\n",
    "            sum += x.path_similarity(y) * math.exp(2.5 * x.path_similarity(y)) + 14 * math.log(0.92+x.path_similarity(y))\n",
    "            count += 1\n",
    "            #print([word1,word2])\n",
    "            #print(str(x.path_similarity(y) * math.exp(3 * x.path_similarity(y)) + 7 * math.log(0.92+x.path_similarity(y))))\n",
    "        except:\n",
    "            if word1 == word2 and re.findall(r\"\\d+.n.01\", word1) == []: #remove years being counted as match yyyy.n.01\n",
    "                sum += 21.32\n",
    "                count += 1\n",
    "                #print([word1,word2,23.3])\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "if count != 0:\n",
    "    percent = round(sum/count*100)\n",
    "    print('Related index is ' + str(percent) + '%')\n",
    "else:\n",
    "    print('No relation index can be calculated as words are all foreign')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page is loading...\n",
      "\n",
      "Search text formatted to title/proper case by default. Set second argument as 'No' to disable formatting\n",
      "Wikipedia page loaded successfully!! Type variablename.HELP() for documentation of functions.\n",
      "\n",
      "\n",
      "Angela Dorothea Merkel (/ˈmɜːrkəl/; German: ; née Kasner; born 17 July 1954) is a German politician serving as Chancellor of Germany since 2005. She served as the leader of the centre-right Christian Democratic Union (CDU) from 2000 to 2018. Merkel has been widely described as the de facto leader of the European Union the most powerful woman in the world, and by many commentators as the leader of the Free World.\n",
      " \n",
      "\n",
      "Merkel was born in Hamburg in then-West Germany and moved to East Germany as an infant when her father, a Lutheran clergyman, received a pastorate in Perleberg. She obtained a doctorate in quantum chemistry in 1986 and worked as a research scientist until 1989. Merkel entered politics in the wake of the Revolutions of 1989 and briefly served as a deputy spokesperson for the first democratically elected East German Government headed by Lothar de Maizière in 1990. Following German reunification in 1990, Merkel was elected to the Bundestag for the state of Mecklenburg-Vorpommern and has been reelected ever since. As the protégée of Chancellor Helmut Kohl Merkel was appointed as the Federal Minister for Women and Youth in Kohl's government in 1991, and became the Federal Minister for the Environment, Nature Conservation and Nuclear Safety in 1994. After her party lost the federal election in 1998 Merkel was elected Secretary-General of the CDU before becoming the party's first female leader two years later in the aftermath of a donations scandal that toppled Wolfgang Schäuble.\n",
      "\n",
      "Page is loading...\n",
      "\n",
      "Search text formatted to title/proper case by default. Set second argument as 'No' to disable formatting\n",
      "Wikipedia page loaded successfully!! Type variablename.HELP() for documentation of functions.\n",
      "\n",
      "\n",
      "Berlin (/bɜːrˈlɪn/; German pronunciation: ) is the capital and largest city of Germany by both area and population. Its 3,748,148 (2018) inhabitants make it the second most populous city proper of the European Union after London. The city is one of Germany's 16 federal states. It is surrounded by the state of Brandenburg and contiguous with its capital, Potsdam. The two cities are at the center of the Berlin-Brandenburg capital region which is, with about six million inhabitants and an area of more than 30,000 km², Germany's third-largest metropolitan region after the Rhine-Ruhr and Rhine-Main regions.\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Page is loading...\n",
      "\n",
      "Search text formatted to title/proper case by default. Set second argument as 'No' to disable formatting\n",
      "Wikipedia page loaded successfully!! Type variablename.HELP() for documentation of functions.\n",
      "\n",
      "\n",
      "\n",
      "Coordinates: 51°N 9°E﻿ / ﻿51°N 9°E﻿ / 51; 9\n",
      " \n",
      "\n",
      "– in Europe (green &amp; dark grey)– in the European Union (green)\n"
     ]
    }
   ],
   "source": [
    "angela = wikiscrape.wiki('angela merkel')\n",
    "berlin = wikiscrape.wiki('berlin')\n",
    "germany = wikiscrape.wiki('germany')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['merkel.n.01', 'angela.n.01', 'german.n.01', '2013.n.01', 'original.n.01', '2017.n.01', 'germany.n.01', 'chancellor.n.01', '/.n.01', '2018.n.01', '2011.n.01', 'cdu.n.01', '2010.n.01', '2016.n.01', 'spd.n.01', 'merkels.n.01', 'election.n.01', '2005.n.01', 'party.n.01', 'der.n.01', '2009.n.01', '2014.n.01', '2008.n.01', 'leader.n.01', '2015.n.01', 'european.n.01', 'coalition.n.01', '2012.n.01', 'isbn.n.01', 'world.n.01', '18.n.01', 'news.n.01', 'von.n.01', 'award.n.01', '23.n.01', 'wikipedia.n.01', 'been.n.01', 'government.n.01', '14.n.01', '2.n.01']\n",
      "['berlin.n.01', 'city.n.01', 'german.n.01', 'germany.n.01', '2008.n.01', 'world.n.01', '2016.n.01', '2015.n.01', '2012.n.01', 'original.n.01', 'berlins.n.01', '2014.n.01', 'capital.n.01', 'largest.n.01', 'west.n.01', 'berliner.n.01', '2013.n.01', 'east.n.01', 'were.n.01', 'der.n.01', '7.n.01', '18.n.01', 'area.n.01', 'european.n.01', 'cities.n.01', 'more.n.01', 'many.n.01', 'isbn.n.01', 'brandenburg.n.01', 'museum.n.01', 'international.n.01', 'around.n.01', 'history.n.01', '10.n.01', 'state.n.01', 'became.n.01', 'europe.n.01', 'western.n.01', '2009.n.01', '2011.n.01']\n",
      "Related index is 74%\n"
     ]
    }
   ],
   "source": [
    "#import nltk\n",
    "#nltk.download('wordnet')\n",
    "#from nltk.corpus import wordnet\n",
    "#wn = nltk.corpus.wordnet #the corpus reader\n",
    "\n",
    "dotn01 = ('.','n','.','0','1')\n",
    "angelalist = []\n",
    "for key in angela.commonwords(40):\n",
    "    angelaslice = list(key)\n",
    "    for letter in dotn01:\n",
    "        angelaslice.append(letter)\n",
    "    angelaslice = ''.join(angelaslice)\n",
    "    angelalist.append(angelaslice)\n",
    "\n",
    "print(angelalist)\n",
    "\n",
    "berlinlist = []\n",
    "for key in berlin.commonwords(40):\n",
    "    berlinslice = list(key)\n",
    "    for letter in dotn01:\n",
    "        berlinslice.append(letter)\n",
    "    berlinslice = ''.join(berlinslice)\n",
    "    berlinlist.append(berlinslice)\n",
    "\n",
    "print(berlinlist)\n",
    "    \n",
    "count = 0\n",
    "sum = 0\n",
    "\n",
    "for word1 in angelalist:\n",
    "    for word2 in berlinlist:\n",
    "        try:\n",
    "            x = wn.synset(word1)\n",
    "            y = wn.synset(word2)\n",
    "            sum += x.path_similarity(y) * math.exp(2.5 * x.path_similarity(y)) + 14 * math.log(0.92+x.path_similarity(y))\n",
    "            count += 1\n",
    "        except:\n",
    "            if word1 == word2 and re.findall(r\"\\d+.n.01\", word1) == []: #remove years being counted as match yyyy.n.01\n",
    "                sum += 21.32\n",
    "                count += 1\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "if count != 0:\n",
    "    percent = round(sum/count*100)\n",
    "    print('Related index is ' + str(percent) + '%')\n",
    "else:\n",
    "    print('No relation index can be calculated as words are all foreign')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['merkel.n.01', 'angela.n.01', 'german.n.01', '2013.n.01', 'original.n.01', '2017.n.01', 'germany.n.01', 'chancellor.n.01', '/.n.01', '2018.n.01', '2011.n.01', 'cdu.n.01', '2010.n.01', '2016.n.01', 'spd.n.01', 'merkels.n.01', 'election.n.01', '2005.n.01', 'party.n.01', 'der.n.01', '2009.n.01', '2014.n.01', '2008.n.01', 'leader.n.01', '2015.n.01', 'european.n.01', 'coalition.n.01', '2012.n.01', 'isbn.n.01', 'world.n.01', '18.n.01', 'news.n.01', 'von.n.01', 'award.n.01', '23.n.01', 'wikipedia.n.01', 'been.n.01', 'government.n.01', '14.n.01', '2.n.01']\n",
      "['trump.n.01', '2018.n.01', '2017.n.01', '2016.n.01', 'donald.n.01', 'trumps.n.01', 'york.n.01', 'times.n.01', '2019.n.01', '2015.n.01', 'president.n.01', 'news.n.01', 'washington.n.01', 'post.n.01', 'had.n.01', 'campaign.n.01', 'presidential.n.01', 'states.n.01', '12.n.01', '3.n.01', 'cnn.n.01', 'michael.n.01', '16.n.01', 'united.n.01', 'election.n.01', '14.n.01', '18.n.01', '20.n.01', 'house.n.01', 'during.n.01', 'said.n.01', '22.n.01', 'been.n.01', '15.n.01', '21.n.01', 'white.n.01', '6.n.01', 'were.n.01', '7.n.01', 'tax.n.01']\n",
      "Related index is 68%\n"
     ]
    }
   ],
   "source": [
    "#import nltk\n",
    "#nltk.download('wordnet')\n",
    "#from nltk.corpus import wordnet\n",
    "#wn = nltk.corpus.wordnet #the corpus reader\n",
    "\n",
    "dotn01 = ('.','n','.','0','1')\n",
    "angelalist = []\n",
    "for key in angela.commonwords(40):\n",
    "    angelaslice = list(key)\n",
    "    for letter in dotn01:\n",
    "        angelaslice.append(letter)\n",
    "    angelaslice = ''.join(angelaslice)\n",
    "    angelalist.append(angelaslice)\n",
    "\n",
    "print(angelalist)\n",
    "\n",
    "#uses Donald Trump List\n",
    "print(trumplist)\n",
    "    \n",
    "count = 0\n",
    "sum = 0\n",
    "\n",
    "for word1 in angelalist:\n",
    "    for word2 in trumplist:\n",
    "        try:\n",
    "            x = wn.synset(word1)\n",
    "            y = wn.synset(word2)\n",
    "            #print([word1,word2])\n",
    "            #print(str(x.path_similarity(y) * math.exp(3.5 * x.path_similarity(y))))\n",
    "            sum += x.path_similarity(y) * math.exp(2.5 * x.path_similarity(y)) + 14 * math.log(0.92+x.path_similarity(y))\n",
    "            count += 1\n",
    "        except:\n",
    "            if word1 == word2 and re.findall(r\"\\d+.n.01\", word1) == []: #remove years being counted as match yyyy.n.01\n",
    "                sum += 21.32\n",
    "                count += 1\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "if count != 0:\n",
    "    percent = round(sum/count*100)\n",
    "    print('Related index is ' + str(percent) + '%')\n",
    "else:\n",
    "    print('No relation index can be calculated as words are all foreign')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['merkel.n.01', 'angela.n.01', 'german.n.01', '2013.n.01', 'original.n.01', '2017.n.01', 'germany.n.01', 'chancellor.n.01', '/.n.01', '2018.n.01', '2011.n.01', 'cdu.n.01', '2010.n.01', '2016.n.01', 'spd.n.01', 'merkels.n.01', 'election.n.01', '2005.n.01', 'party.n.01', 'der.n.01', '2009.n.01', '2014.n.01', '2008.n.01', 'leader.n.01', '2015.n.01', 'european.n.01', 'coalition.n.01', '2012.n.01', 'isbn.n.01', 'world.n.01', '18.n.01', 'news.n.01', 'von.n.01', 'award.n.01', '23.n.01', 'wikipedia.n.01', 'been.n.01', 'government.n.01', '14.n.01', '2.n.01']\n",
      "['obama.n.01', '2008.n.01', '2009.n.01', 'original.n.01', '2010.n.01', '2011.n.01', 'barack.n.01', '2007.n.01', '2012.n.01', 'obamas.n.01', 'chicago.n.01', 'york.n.01', 'president.n.01', 'times.n.01', '2017.n.01', '2015.n.01', 'pp.n.01', 'campaign.n.01', '3.n.01', '2004.n.01', 'united.n.01', '/.n.01', '2016.n.01', '2014.n.01', 'act.n.01', '15.n.01', '20.n.01', '5.n.01', 'senate.n.01', '1.n.01', 'law.n.01', '2013.n.01', 'states.n.01', '18.n.01', 'washington.n.01', '9.n.01', '27.n.01', 'house.n.01', '7.n.01', '24.n.01']\n",
      "Related index is 60%\n"
     ]
    }
   ],
   "source": [
    "#import nltk\n",
    "#nltk.download('wordnet')\n",
    "#from nltk.corpus import wordnet\n",
    "#wn = nltk.corpus.wordnet #the corpus reader\n",
    "\n",
    "dotn01 = ('.','n','.','0','1')\n",
    "angelalist = []\n",
    "for key in angela.commonwords(40):\n",
    "    angelaslice = list(key)\n",
    "    for letter in dotn01:\n",
    "        angelaslice.append(letter)\n",
    "    angelaslice = ''.join(angelaslice)\n",
    "    angelalist.append(angelaslice)\n",
    "\n",
    "print(angelalist)\n",
    "\n",
    "obamalist = []\n",
    "for key in obama.commonwords(40):\n",
    "    obamaslice = list(key)\n",
    "    for letter in dotn01:\n",
    "        obamaslice.append(letter)\n",
    "    obamaslice = ''.join(obamaslice)\n",
    "    obamalist.append(obamaslice)\n",
    "    \n",
    "print(obamalist)\n",
    "    \n",
    "count = 0\n",
    "sum = 0\n",
    "\n",
    "for word1 in angelalist:\n",
    "    for word2 in obamalist:\n",
    "        try:\n",
    "            x = wn.synset(word1)\n",
    "            y = wn.synset(word2)\n",
    "            sum += x.path_similarity(y) * math.exp(2.5 * x.path_similarity(y)) + 14 * math.log(0.92+x.path_similarity(y))\n",
    "            count += 1\n",
    "        except:\n",
    "            if word1 == word2 and re.findall(r\"\\d+.n.01\", word1) == []: #remove years being counted as match yyyy.n.01\n",
    "                sum += 21.32\n",
    "                count += 1\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "if count != 0:\n",
    "    percent = round(sum/count*100)\n",
    "    print('Related index is ' + str(percent) + '%')\n",
    "else:\n",
    "    print('No relation index can be calculated as words are all foreign')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page is loading...\n",
      "\n",
      "Search text formatted to title/proper case by default. Set second argument as 'No' to disable formatting\n",
      "Wikipedia page loaded successfully!! Type variablename.HELP() for documentation of functions.\n",
      "\n",
      "\n",
      "Vincent Willem van Gogh (Dutch:  (listen); 30 March 1853 – 29 July 1890) was a Dutch Post-Impressionist painter who is among the most famous and influential figures in the history of Western art. In just over a decade he created about 2,100 artworks, including around 860 oil paintings most of them in the last two years of his life. They include landscapes still lifes portraits and self-portraits and are characterised by bold colours and dramatic, impulsive and expressive brushwork that contributed to the foundations of modern art. However, he was not commercially successful, and his suicide at 37 followed years of mental illness and poverty.\n",
      " \n",
      "\n",
      "Born into an upper-middle-class family, Van Gogh drew as a child and was serious, quiet and thoughtful. As a young man he worked as an art dealer, often travelling, but became depressed after he was transferred to London. He turned to religion and spent time as a Protestant missionary in southern Belgium. He drifted in ill health and solitude before taking up painting in 1881, having moved back home with his parents. His younger brother Theo supported him financially, and the two kept up a long correspondence by letter. His early works, mostly still lifes and depictions of peasant labourers contain few signs of the vivid colour that distinguished his later work. In 1886, he moved to Paris, where he met members of the avant-garde including Émile Bernard and Paul Gauguin who were reacting against the Impressionist sensibility. As his work developed he created a new approach to still lifes and local landscapes. His paintings grew brighter in colour as he developed a style that became fully realised during his stay in Arles in the south of France in 1888. During this period he broadened his subject matter to include series of olive trees wheat fields and sunflowers.\n",
      "\n",
      "Page is loading...\n",
      "\n",
      "Search text formatted to title/proper case by default. Set second argument as 'No' to disable formatting\n",
      "Wikipedia page loaded successfully!! Type variablename.HELP() for documentation of functions.\n",
      "\n",
      "\n",
      "Banksy is an anonymous England-based street artist, vandal, political activist, and film director. His satirical street art and subversive epigrams combine dark humour with graffiti executed in a distinctive stenciling technique. His works of political and social commentary have been featured on streets, walls, and bridges of cities throughout the world. Banksy's work grew out of the Bristol underground scene which involved collaborations between artists and musicians. Banksy says that he was inspired by 3D a graffiti artist who later became a founding member of the English musical group Massive Attack.\n",
      " \n",
      "\n",
      "Banksy displays his art on publicly visible surfaces such as walls and self-built physical prop pieces. Banksy no longer sells photographs or reproductions of his street graffiti, but his public 'installations' are regularly resold, often even by removing the wall they were painted on. A small number of Banksy's works are officially, non-publicly, sold through Pest Control. Banksy's documentary film Exit Through the Gift Shop (2010) made its debut at the 2010 Sundance Film Festival. In January 2011, he was nominated for the Academy Award for Best Documentary for the film. In 2014, he was awarded Person of the Year at the 2014 Webby Awards.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vangogh = wikiscrape.wiki('van gogh')\n",
    "banksy = wikiscrape.wiki('banksy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['van.n.01', 'gogh.n.01', 'vincent.n.01', 'museum.n.01', 'theo.n.01', '&amp.n.01', 'art.n.01', 'goghs.n.01', '1888.n.01', '1990.n.01', 'arles.n.01', 'paintings.n.01', 'had.n.01', 'isbn.n.01', '1890.n.01', '1889.n.01', 'life.n.01', 'paris.n.01', 'letter.n.01', 'painted.n.01', 'gauguin.n.01', 'were.n.01', 'amsterdam.n.01', 'painting.n.01', '2009.n.01', '2011.n.01', 'de.n.01', 'works.n.01', 'wheat.n.01', 'naifeh.n.01', 'smith.n.01', 'pickvance.n.01', '1981.n.01', 'hulsker.n.01', 'still.n.01', 'wikipedia.n.01', 'dutch.n.01', 'portrait.n.01', 'paul.n.01', 'during.n.01']\n",
      "['banksy.n.01', 'original.n.01', 'art.n.01', '2013.n.01', 'banksys.n.01', '2006.n.01', 'bristol.n.01', 'london.n.01', 'news.n.01', 'bbc.n.01', 'graffiti.n.01', '2009.n.01', '2010.n.01', '2008.n.01', '2018.n.01', 'artist.n.01', '2011.n.01', '2014.n.01', '2007.n.01', 'street.n.01', '2016.n.01', 'wall.n.01', 'auction.n.01', '2012.n.01', '2015.n.01', '19.n.01', 'artists.n.01', 'york.n.01', 'works.n.01', 'church.n.01', 'st.n.01', 'painted.n.01', 'been.n.01', 'were.n.01', 'house.n.01', '21.n.01', 'piece.n.01', '17.n.01', 'had.n.01', 'mural.n.01']\n",
      "Related index is 56%\n"
     ]
    }
   ],
   "source": [
    "#import nltk\n",
    "#nltk.download('wordnet')\n",
    "#from nltk.corpus import wordnet\n",
    "#wn = nltk.corpus.wordnet #the corpus reader\n",
    "\n",
    "dotn01 = ('.','n','.','0','1')\n",
    "vangoghlist = []\n",
    "for key in vangogh.commonwords(40):\n",
    "    vangoghslice = list(key)\n",
    "    for letter in dotn01:\n",
    "        vangoghslice.append(letter)\n",
    "    vangoghslice = ''.join(vangoghslice)\n",
    "    vangoghlist.append(vangoghslice)\n",
    "\n",
    "print(vangoghlist)\n",
    "\n",
    "banksylist = []\n",
    "for key in banksy.commonwords(40):\n",
    "    banksyslice = list(key)\n",
    "    for letter in dotn01:\n",
    "        banksyslice.append(letter)\n",
    "    banksyslice = ''.join(banksyslice)\n",
    "    banksylist.append(banksyslice)\n",
    "    \n",
    "print(banksylist)\n",
    "    \n",
    "count = 0\n",
    "sum = 0\n",
    "\n",
    "for word1 in vangoghlist:\n",
    "    for word2 in banksylist:\n",
    "        try:\n",
    "            x = wn.synset(word1)\n",
    "            y = wn.synset(word2)\n",
    "            sum += x.path_similarity(y) * math.exp(2.5 * x.path_similarity(y)) + 14 * math.log(0.92+x.path_similarity(y))\n",
    "            #print([word1,word2])\n",
    "            #print(str(x.path_similarity(y) * math.exp(3 * x.path_similarity(y))))\n",
    "            count += 1\n",
    "        except:\n",
    "            if word1 == word2 and re.findall(r\"\\d+.n.01\", word1) == []: #remove years being counted as match yyyy.n.01\n",
    "                sum += 21.32\n",
    "                count += 1\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "if count != 0:\n",
    "    percent = round(sum/count*100)\n",
    "    print('Related index is ' + str(percent) + '%')\n",
    "else:\n",
    "    print('No relation index can be calculated as words are all foreign')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nickelback.n.01', 'album.n.01', 'band.n.01', '2012.n.01', 'original.n.01', 'rock.n.01', '2011.n.01', '2010.n.01', 'released.n.01', 'kroeger.n.01', 'billboard.n.01', '2008.n.01', 'single.n.01', 'tour.n.01', '5.n.01', 'number.n.01', 'music.n.01', 'chad.n.01', 'dark.n.01', 'top.n.01', 'horse.n.01', '3.n.01', 'song.n.01', '2015.n.01', 'canadian.n.01', '2009.n.01', '28.n.01', 'wikipedia.n.01', 'bands.n.01', 'albums.n.01', 'roadrunner.n.01', 'records.n.01', 'release.n.01', 'singles.n.01', 'here.n.01', 'award.n.01', 'socan.n.01', '2013.n.01', 'peaked.n.01', '1.n.01']\n",
      "['banksy.n.01', 'original.n.01', 'art.n.01', '2013.n.01', 'banksys.n.01', '2006.n.01', 'bristol.n.01', 'london.n.01', 'news.n.01', 'bbc.n.01', 'graffiti.n.01', '2009.n.01', '2010.n.01', '2008.n.01', '2018.n.01', 'artist.n.01', '2011.n.01', '2014.n.01', '2007.n.01', 'street.n.01', '2016.n.01', 'wall.n.01', 'auction.n.01', '2012.n.01', '2015.n.01', '19.n.01', 'artists.n.01', 'york.n.01', 'works.n.01', 'church.n.01', 'st.n.01', 'painted.n.01', 'been.n.01', 'were.n.01', 'house.n.01', '21.n.01', 'piece.n.01', '17.n.01', 'had.n.01', 'mural.n.01']\n",
      "Related index is 27%\n"
     ]
    }
   ],
   "source": [
    "#import nltk\n",
    "#nltk.download('wordnet')\n",
    "#from nltk.corpus import wordnet\n",
    "#wn = nltk.corpus.wordnet #the corpus reader\n",
    "\n",
    "dotn01 = ('.','n','.','0','1')\n",
    "nickelbacklist = []\n",
    "for key in nickelback.commonwords(40):\n",
    "    nickelbackslice = list(key)\n",
    "    for letter in dotn01:\n",
    "        nickelbackslice.append(letter)\n",
    "    nickelbackslice = ''.join(nickelbackslice)\n",
    "    nickelbacklist.append(nickelbackslice)\n",
    "\n",
    "print(nickelbacklist)\n",
    "\n",
    "#use banksy list\n",
    "\n",
    "print(banksylist)\n",
    "    \n",
    "count = 0\n",
    "sum = 0\n",
    "\n",
    "for word1 in nickelbacklist:\n",
    "    for word2 in banksylist:\n",
    "        try:\n",
    "            x = wn.synset(word1)\n",
    "            y = wn.synset(word2)\n",
    "            sum += x.path_similarity(y) * math.exp(2.5 * x.path_similarity(y)) + 14 * math.log(0.92+x.path_similarity(y))\n",
    "            count += 1\n",
    "        except:\n",
    "            if word1 == word2 and re.findall(r\"\\d+.n.01\", word1) == []: #remove years being counted as match yyyy.n.01\n",
    "                sum += 21.32\n",
    "                count += 1\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "if count != 0:\n",
    "    percent = round(sum/count*100)\n",
    "    print('Related index is ' + str(percent) + '%')\n",
    "else:\n",
    "    print('No relation index can be calculated as words are all foreign')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page is loading...\n",
      "\n",
      "Search text formatted to title/proper case by default. Set second argument as 'No' to disable formatting\n",
      "Wikipedia page loaded successfully!! Type variablename.HELP() for documentation of functions.\n",
      "\n",
      "\n",
      "The Goldman Sachs Group, Inc. is an American multinational investment bank and financial services company headquartered in New York City. It offers services in investment management securities, asset management prime brokerage and securities underwriting.\n",
      " \n",
      "\n",
      "The bank is one of the largest investment banking enterprises in the world, and is a primary dealer in the United States Treasury security market and more generally, a prominent market maker. The bank also owns Goldman Sachs Bank USA, a direct bank. Goldman Sachs was founded in 1869 and is headquartered at 200 West Street in Lower Manhattan with additional offices in other international financial centers.\n",
      "\n",
      "Page is loading...\n",
      "\n",
      "Search text has preserved the cases of each letter. Set second argument as 'Yes' to format to title/proper case\n",
      "Wikipedia page loaded successfully!! Type variablename.HELP() for documentation of functions.\n",
      "\n",
      "\n",
      "DBS Bank is a multinational banking and financial services corporation headquartered in Marina Bay Financial Centre Tower 3 Marina Bay, Singapore. The company was known as The Development Bank of Singapore Limited before the present name was adopted in July 2003 to reflect its changing role as a regional bank.\n",
      " \n",
      "\n",
      "The bank was set up by the Government of Singapore in July 1968 to take over the industrial financing activities from the Economic Development Board. Today, its branches numbering more than 100 can be found island-wide. DBS Bank is the largest bank in South East Asia by assets and among the larger banks in Asia, with total assets of S$518 billion as at 31 Dec 2017. It has market-dominant positions in consumer banking, treasury and markets, asset management, securities brokerage, equity and debt fund-raising in Singapore and Hong Kong.\n",
      "DBS Bank's largest, and controlling, shareholder is Temasek Holdings Singapore's second largest sovereign wealth fund (after GIC). As of 31 March 2018 Temasek owns 29% of DBS' shares.\n",
      "The bank's strong capital position, as well as \"AA-\" and \"Aa1\" credit ratings by Standard &amp; Poor's and Moody's that are among the highest in the Asia-Pacific region, earned it Global Finance's \"Safest Bank in Asia\" accolade for six consecutive years, from 2009 to 2015. The Bank was also awarded the Best Digital Bank in the World in the year 2016 by Euromoney. With operations in 17 markets, the bank has a regional network spanning more than 250 branches and over 1,100 ATMs across 50 cities.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "goldman = wikiscrape.wiki('goldman sachs')\n",
    "dbs = wikiscrape.wiki('DBS bank','no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['van.n.01', 'gogh.n.01', 'vincent.n.01', 'museum.n.01', 'theo.n.01', '&amp.n.01', 'art.n.01', 'goghs.n.01', '1888.n.01', '1990.n.01', 'arles.n.01', 'paintings.n.01', 'had.n.01', 'isbn.n.01', '1890.n.01', '1889.n.01', 'life.n.01', 'paris.n.01', 'letter.n.01', 'painted.n.01', 'gauguin.n.01', 'were.n.01', 'amsterdam.n.01', 'painting.n.01', '2009.n.01', '2011.n.01', 'de.n.01', 'works.n.01', 'wheat.n.01', 'naifeh.n.01', 'smith.n.01', 'pickvance.n.01', '1981.n.01', 'hulsker.n.01', 'still.n.01', 'wikipedia.n.01', 'dutch.n.01', 'portrait.n.01', 'paul.n.01', 'during.n.01']\n",
      "['goldman.n.01', 'sachs.n.01', 'york.n.01', 'securities.n.01', 'investment.n.01', 'bank.n.01', 'billion.n.01', '2010.n.01', '2013.n.01', 'firm.n.01', 'times.n.01', 'million.n.01', 'street.n.01', 'were.n.01', 'financial.n.01', 'had.n.01', '2008.n.01', '2011.n.01', 'company.n.01', '2009.n.01', 'banks.n.01', 'wall.n.01', 'help.n.01', 'subscription.n.01', 'required.n.01', 'business.n.01', 'goldmans.n.01', 'crisis.n.01', 'fund.n.01', '&amp.n.01', 'trading.n.01', 'former.n.01', 'employees.n.01', 'deal.n.01', 'investors.n.01', '2014.n.01', 'market.n.01', 'bloomberg.n.01', 'abacus.n.01', 'sec.n.01']\n",
      "Related index is 19%\n"
     ]
    }
   ],
   "source": [
    "#import nltk\n",
    "#nltk.download('wordnet')\n",
    "#from nltk.corpus import wordnet\n",
    "#wn = nltk.corpus.wordnet #the corpus reader\n",
    "\n",
    "dotn01 = ('.','n','.','0','1')\n",
    "vangoghlist = []\n",
    "for key in vangogh.commonwords(40):\n",
    "    vangoghslice = list(key)\n",
    "    for letter in dotn01:\n",
    "        vangoghslice.append(letter)\n",
    "    vangoghslice = ''.join(vangoghslice)\n",
    "    vangoghlist.append(vangoghslice)\n",
    "\n",
    "print(vangoghlist)\n",
    "\n",
    "goldmanlist = []\n",
    "for key in goldman.commonwords(40):\n",
    "    goldmanslice = list(key)\n",
    "    for letter in dotn01:\n",
    "        goldmanslice.append(letter)\n",
    "    goldmanslice = ''.join(goldmanslice)\n",
    "    goldmanlist.append(goldmanslice)\n",
    "    \n",
    "print(goldmanlist)\n",
    "    \n",
    "count = 0\n",
    "sum = 0\n",
    "\n",
    "for word1 in vangoghlist:\n",
    "    for word2 in goldmanlist:\n",
    "        try:\n",
    "            x = wn.synset(word1)\n",
    "            y = wn.synset(word2)\n",
    "            sum += x.path_similarity(y) * math.exp(2.5 * x.path_similarity(y)) + 14 * math.log(0.92+x.path_similarity(y))\n",
    "            #print([word1,word2])\n",
    "            #print(str(x.path_similarity(y) * math.exp(3 * x.path_similarity(y))))\n",
    "            count += 1\n",
    "        except:\n",
    "            if word1 == word2 and re.findall(r\"\\d+.n.01\", word1) == []: #remove years being counted as match yyyy.n.01\n",
    "                sum += 21.32\n",
    "                count += 1\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "if count != 0:\n",
    "    percent = round(sum/count*100)\n",
    "    print('Related index is ' + str(percent) + '%')\n",
    "else:\n",
    "    print('No relation index can be calculated as words are all foreign')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bank.n.01', 'dbs.n.01', 'singapore.n.01', 'banking.n.01', 'banks.n.01', 'holdings.n.01', 'china.n.01', 'commercial.n.01', 'development.n.01', 'temasek.n.01', 'trust.n.01', 'branches.n.01', 'asia.n.01', 'posb.n.01', 'financial.n.01', 'best.n.01', 'customers.n.01', 'limited.n.01', '1968.n.01', 'industrial.n.01', 'shares.n.01', '2016.n.01', 'united.n.01', 'india.n.01', 'indonesia.n.01', 'taiwan.n.01', 'mbanking.n.01', 'corporation.n.01', 'government.n.01', 'billion.n.01', '2017.n.01', 'hong.n.01', '&amp.n.01', 'global.n.01', '2015.n.01', 'finance.n.01', 'programme.n.01', 'device.n.01', 'hotspot.n.01', 'startups.n.01']\n",
      "['goldman.n.01', 'sachs.n.01', 'york.n.01', 'securities.n.01', 'investment.n.01', 'bank.n.01', 'billion.n.01', '2010.n.01', '2013.n.01', 'firm.n.01', 'times.n.01', 'million.n.01', 'street.n.01', 'were.n.01', 'financial.n.01', 'had.n.01', '2008.n.01', '2011.n.01', 'company.n.01', '2009.n.01', 'banks.n.01', 'wall.n.01', 'help.n.01', 'subscription.n.01', 'required.n.01', 'business.n.01', 'goldmans.n.01', 'crisis.n.01', 'fund.n.01', '&amp.n.01', 'trading.n.01', 'former.n.01', 'employees.n.01', 'deal.n.01', 'investors.n.01', '2014.n.01', 'market.n.01', 'bloomberg.n.01', 'abacus.n.01', 'sec.n.01']\n",
      "['bank.n.01', 'goldman.n.01', 0.1]\n",
      "['bank.n.01', 'york.n.01', 0.06666666666666667]\n",
      "['bank.n.01', 'investment.n.01', 0.06666666666666667]\n",
      "['bank.n.01', 'bank.n.01', 1.0]\n",
      "['bank.n.01', 'billion.n.01', 0.07692307692307693]\n",
      "['bank.n.01', 'firm.n.01', 0.07692307692307693]\n",
      "['bank.n.01', 'times.n.01', 0.09090909090909091]\n",
      "['bank.n.01', 'million.n.01', 0.07692307692307693]\n",
      "['bank.n.01', 'street.n.01', 0.1]\n",
      "['bank.n.01', 'company.n.01', 0.08333333333333333]\n",
      "['bank.n.01', 'banks.n.01', 0.09090909090909091]\n",
      "['bank.n.01', 'wall.n.01', 0.1111111111111111]\n",
      "['bank.n.01', 'help.n.01', 0.08333333333333333]\n",
      "['bank.n.01', 'subscription.n.01', 0.07142857142857142]\n",
      "['bank.n.01', 'business.n.01', 0.08333333333333333]\n",
      "['bank.n.01', 'crisis.n.01', 0.07692307692307693]\n",
      "['bank.n.01', 'fund.n.01', 0.07692307692307693]\n",
      "['bank.n.01', 'trading.n.01', 0.07692307692307693]\n",
      "['bank.n.01', 'former.n.01', 0.07692307692307693]\n",
      "['bank.n.01', 'deal.n.01', 0.08333333333333333]\n",
      "['bank.n.01', 'market.n.01', 0.08333333333333333]\n",
      "['bank.n.01', 'abacus.n.01', 0.1]\n",
      "['bank.n.01', 'sec.n.01', 0.1]\n",
      "['singapore.n.01', 'goldman.n.01', 0.06666666666666667]\n",
      "['singapore.n.01', 'york.n.01', 0.05]\n",
      "['singapore.n.01', 'investment.n.01', 0.05]\n",
      "['singapore.n.01', 'bank.n.01', 0.08333333333333333]\n",
      "['singapore.n.01', 'billion.n.01', 0.05555555555555555]\n",
      "['singapore.n.01', 'firm.n.01', 0.05555555555555555]\n",
      "['singapore.n.01', 'times.n.01', 0.0625]\n",
      "['singapore.n.01', 'million.n.01', 0.05555555555555555]\n",
      "['singapore.n.01', 'street.n.01', 0.06666666666666667]\n",
      "['singapore.n.01', 'company.n.01', 0.058823529411764705]\n",
      "['singapore.n.01', 'banks.n.01', 0.0625]\n",
      "['singapore.n.01', 'wall.n.01', 0.07142857142857142]\n",
      "['singapore.n.01', 'help.n.01', 0.058823529411764705]\n",
      "['singapore.n.01', 'subscription.n.01', 0.05263157894736842]\n",
      "['singapore.n.01', 'business.n.01', 0.058823529411764705]\n",
      "['singapore.n.01', 'crisis.n.01', 0.05555555555555555]\n",
      "['singapore.n.01', 'fund.n.01', 0.05555555555555555]\n",
      "['singapore.n.01', 'trading.n.01', 0.05555555555555555]\n",
      "['singapore.n.01', 'former.n.01', 0.05555555555555555]\n",
      "['singapore.n.01', 'deal.n.01', 0.058823529411764705]\n",
      "['singapore.n.01', 'market.n.01', 0.058823529411764705]\n",
      "['singapore.n.01', 'abacus.n.01', 0.06666666666666667]\n",
      "['singapore.n.01', 'sec.n.01', 0.06666666666666667]\n",
      "['banking.n.01', 'goldman.n.01', 0.06666666666666667]\n",
      "['banking.n.01', 'york.n.01', 0.0625]\n",
      "['banking.n.01', 'investment.n.01', 0.1]\n",
      "['banking.n.01', 'bank.n.01', 0.07142857142857142]\n",
      "['banking.n.01', 'billion.n.01', 0.07142857142857142]\n",
      "['banking.n.01', 'firm.n.01', 0.07142857142857142]\n",
      "['banking.n.01', 'times.n.01', 0.08333333333333333]\n",
      "['banking.n.01', 'million.n.01', 0.07142857142857142]\n",
      "['banking.n.01', 'street.n.01', 0.058823529411764705]\n",
      "['banking.n.01', 'company.n.01', 0.07692307692307693]\n",
      "['banking.n.01', 'banks.n.01', 0.0625]\n",
      "['banking.n.01', 'wall.n.01', 0.0625]\n",
      "['banking.n.01', 'help.n.01', 0.125]\n",
      "['banking.n.01', 'subscription.n.01', 0.06666666666666667]\n",
      "['banking.n.01', 'business.n.01', 0.07692307692307693]\n",
      "['banking.n.01', 'crisis.n.01', 0.07142857142857142]\n",
      "['banking.n.01', 'fund.n.01', 0.07142857142857142]\n",
      "['banking.n.01', 'trading.n.01', 0.125]\n",
      "['banking.n.01', 'former.n.01', 0.07142857142857142]\n",
      "['banking.n.01', 'deal.n.01', 0.14285714285714285]\n",
      "['banking.n.01', 'market.n.01', 0.125]\n",
      "['banking.n.01', 'abacus.n.01', 0.058823529411764705]\n",
      "['banking.n.01', 'sec.n.01', 0.09090909090909091]\n",
      "['banks.n.01', 'goldman.n.01', 0.125]\n",
      "['banks.n.01', 'york.n.01', 0.058823529411764705]\n",
      "['banks.n.01', 'investment.n.01', 0.058823529411764705]\n",
      "['banks.n.01', 'bank.n.01', 0.09090909090909091]\n",
      "['banks.n.01', 'billion.n.01', 0.06666666666666667]\n",
      "['banks.n.01', 'firm.n.01', 0.06666666666666667]\n",
      "['banks.n.01', 'times.n.01', 0.07692307692307693]\n",
      "['banks.n.01', 'million.n.01', 0.06666666666666667]\n",
      "['banks.n.01', 'street.n.01', 0.07692307692307693]\n",
      "['banks.n.01', 'company.n.01', 0.07142857142857142]\n",
      "['banks.n.01', 'banks.n.01', 1.0]\n",
      "['banks.n.01', 'wall.n.01', 0.08333333333333333]\n",
      "['banks.n.01', 'help.n.01', 0.07142857142857142]\n",
      "['banks.n.01', 'subscription.n.01', 0.0625]\n",
      "['banks.n.01', 'business.n.01', 0.07142857142857142]\n",
      "['banks.n.01', 'crisis.n.01', 0.06666666666666667]\n",
      "['banks.n.01', 'fund.n.01', 0.06666666666666667]\n",
      "['banks.n.01', 'trading.n.01', 0.06666666666666667]\n",
      "['banks.n.01', 'former.n.01', 0.06666666666666667]\n",
      "['banks.n.01', 'deal.n.01', 0.07142857142857142]\n",
      "['banks.n.01', 'market.n.01', 0.07142857142857142]\n",
      "['banks.n.01', 'abacus.n.01', 0.07692307692307693]\n",
      "['banks.n.01', 'sec.n.01', 0.08333333333333333]\n",
      "['china.n.01', 'goldman.n.01', 0.07142857142857142]\n",
      "['china.n.01', 'york.n.01', 0.05263157894736842]\n",
      "['china.n.01', 'investment.n.01', 0.05263157894736842]\n",
      "['china.n.01', 'bank.n.01', 0.09090909090909091]\n",
      "['china.n.01', 'billion.n.01', 0.058823529411764705]\n",
      "['china.n.01', 'firm.n.01', 0.058823529411764705]\n",
      "['china.n.01', 'times.n.01', 0.06666666666666667]\n",
      "['china.n.01', 'million.n.01', 0.058823529411764705]\n",
      "['china.n.01', 'street.n.01', 0.07142857142857142]\n",
      "['china.n.01', 'company.n.01', 0.0625]\n",
      "['china.n.01', 'banks.n.01', 0.06666666666666667]\n",
      "['china.n.01', 'wall.n.01', 0.07692307692307693]\n",
      "['china.n.01', 'help.n.01', 0.0625]\n",
      "['china.n.01', 'subscription.n.01', 0.05555555555555555]\n",
      "['china.n.01', 'business.n.01', 0.0625]\n",
      "['china.n.01', 'crisis.n.01', 0.058823529411764705]\n",
      "['china.n.01', 'fund.n.01', 0.058823529411764705]\n",
      "['china.n.01', 'trading.n.01', 0.058823529411764705]\n",
      "['china.n.01', 'former.n.01', 0.058823529411764705]\n",
      "['china.n.01', 'deal.n.01', 0.0625]\n",
      "['china.n.01', 'market.n.01', 0.0625]\n",
      "['china.n.01', 'abacus.n.01', 0.07142857142857142]\n",
      "['china.n.01', 'sec.n.01', 0.07142857142857142]\n",
      "['commercial.n.01', 'goldman.n.01', 0.07692307692307693]\n",
      "['commercial.n.01', 'york.n.01', 0.07142857142857142]\n",
      "['commercial.n.01', 'investment.n.01', 0.07142857142857142]\n",
      "['commercial.n.01', 'bank.n.01', 0.08333333333333333]\n",
      "['commercial.n.01', 'billion.n.01', 0.08333333333333333]\n",
      "['commercial.n.01', 'firm.n.01', 0.08333333333333333]\n",
      "['commercial.n.01', 'times.n.01', 0.1]\n",
      "['commercial.n.01', 'million.n.01', 0.08333333333333333]\n",
      "['commercial.n.01', 'street.n.01', 0.06666666666666667]\n",
      "['commercial.n.01', 'company.n.01', 0.09090909090909091]\n",
      "['commercial.n.01', 'banks.n.01', 0.07142857142857142]\n",
      "['commercial.n.01', 'wall.n.01', 0.07142857142857142]\n",
      "['commercial.n.01', 'help.n.01', 0.09090909090909091]\n",
      "['commercial.n.01', 'subscription.n.01', 0.07692307692307693]\n",
      "['commercial.n.01', 'business.n.01', 0.09090909090909091]\n",
      "['commercial.n.01', 'crisis.n.01', 0.08333333333333333]\n",
      "['commercial.n.01', 'fund.n.01', 0.08333333333333333]\n",
      "['commercial.n.01', 'trading.n.01', 0.08333333333333333]\n",
      "['commercial.n.01', 'former.n.01', 0.08333333333333333]\n",
      "['commercial.n.01', 'deal.n.01', 0.09090909090909091]\n",
      "['commercial.n.01', 'market.n.01', 0.09090909090909091]\n",
      "['commercial.n.01', 'abacus.n.01', 0.06666666666666667]\n",
      "['commercial.n.01', 'sec.n.01', 0.1111111111111111]\n",
      "['development.n.01', 'goldman.n.01', 0.0625]\n",
      "['development.n.01', 'york.n.01', 0.058823529411764705]\n",
      "['development.n.01', 'investment.n.01', 0.08333333333333333]\n",
      "['development.n.01', 'bank.n.01', 0.06666666666666667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['development.n.01', 'billion.n.01', 0.06666666666666667]\n",
      "['development.n.01', 'firm.n.01', 0.06666666666666667]\n",
      "['development.n.01', 'times.n.01', 0.07692307692307693]\n",
      "['development.n.01', 'million.n.01', 0.06666666666666667]\n",
      "['development.n.01', 'street.n.01', 0.05555555555555555]\n",
      "['development.n.01', 'company.n.01', 0.07142857142857142]\n",
      "['development.n.01', 'banks.n.01', 0.058823529411764705]\n",
      "['development.n.01', 'wall.n.01', 0.058823529411764705]\n",
      "['development.n.01', 'help.n.01', 0.125]\n",
      "['development.n.01', 'subscription.n.01', 0.0625]\n",
      "['development.n.01', 'business.n.01', 0.07142857142857142]\n",
      "['development.n.01', 'crisis.n.01', 0.06666666666666667]\n",
      "['development.n.01', 'fund.n.01', 0.06666666666666667]\n",
      "['development.n.01', 'trading.n.01', 0.1]\n",
      "['development.n.01', 'former.n.01', 0.06666666666666667]\n",
      "['development.n.01', 'deal.n.01', 0.1111111111111111]\n",
      "['development.n.01', 'market.n.01', 0.125]\n",
      "['development.n.01', 'abacus.n.01', 0.05555555555555555]\n",
      "['development.n.01', 'sec.n.01', 0.08333333333333333]\n",
      "['trust.n.01', 'goldman.n.01', 0.08333333333333333]\n",
      "['trust.n.01', 'york.n.01', 0.07692307692307693]\n",
      "['trust.n.01', 'investment.n.01', 0.07692307692307693]\n",
      "['trust.n.01', 'bank.n.01', 0.09090909090909091]\n",
      "['trust.n.01', 'billion.n.01', 0.09090909090909091]\n",
      "['trust.n.01', 'firm.n.01', 0.09090909090909091]\n",
      "['trust.n.01', 'times.n.01', 0.1111111111111111]\n",
      "['trust.n.01', 'million.n.01', 0.09090909090909091]\n",
      "['trust.n.01', 'street.n.01', 0.07142857142857142]\n",
      "['trust.n.01', 'company.n.01', 0.1]\n",
      "['trust.n.01', 'banks.n.01', 0.07692307692307693]\n",
      "['trust.n.01', 'wall.n.01', 0.07692307692307693]\n",
      "['trust.n.01', 'help.n.01', 0.1]\n",
      "['trust.n.01', 'subscription.n.01', 0.125]\n",
      "['trust.n.01', 'business.n.01', 0.1]\n",
      "['trust.n.01', 'crisis.n.01', 0.09090909090909091]\n",
      "['trust.n.01', 'fund.n.01', 0.09090909090909091]\n",
      "['trust.n.01', 'trading.n.01', 0.09090909090909091]\n",
      "['trust.n.01', 'former.n.01', 0.09090909090909091]\n",
      "['trust.n.01', 'deal.n.01', 0.1]\n",
      "['trust.n.01', 'market.n.01', 0.1]\n",
      "['trust.n.01', 'abacus.n.01', 0.07142857142857142]\n",
      "['trust.n.01', 'sec.n.01', 0.125]\n",
      "['asia.n.01', 'goldman.n.01', 0.09090909090909091]\n",
      "['asia.n.01', 'york.n.01', 0.0625]\n",
      "['asia.n.01', 'investment.n.01', 0.0625]\n",
      "['asia.n.01', 'bank.n.01', 0.125]\n",
      "['asia.n.01', 'billion.n.01', 0.07142857142857142]\n",
      "['asia.n.01', 'firm.n.01', 0.07142857142857142]\n",
      "['asia.n.01', 'times.n.01', 0.08333333333333333]\n",
      "['asia.n.01', 'million.n.01', 0.07142857142857142]\n",
      "['asia.n.01', 'street.n.01', 0.09090909090909091]\n",
      "['asia.n.01', 'company.n.01', 0.07692307692307693]\n",
      "['asia.n.01', 'banks.n.01', 0.08333333333333333]\n",
      "['asia.n.01', 'wall.n.01', 0.1]\n",
      "['asia.n.01', 'help.n.01', 0.07692307692307693]\n",
      "['asia.n.01', 'subscription.n.01', 0.06666666666666667]\n",
      "['asia.n.01', 'business.n.01', 0.07692307692307693]\n",
      "['asia.n.01', 'crisis.n.01', 0.07142857142857142]\n",
      "['asia.n.01', 'fund.n.01', 0.07142857142857142]\n",
      "['asia.n.01', 'trading.n.01', 0.07142857142857142]\n",
      "['asia.n.01', 'former.n.01', 0.07142857142857142]\n",
      "['asia.n.01', 'deal.n.01', 0.07692307692307693]\n",
      "['asia.n.01', 'market.n.01', 0.07692307692307693]\n",
      "['asia.n.01', 'abacus.n.01', 0.09090909090909091]\n",
      "['asia.n.01', 'sec.n.01', 0.09090909090909091]\n",
      "['financial.n.01', 'financial.n.01', 1]\n",
      "['best.n.01', 'goldman.n.01', 0.07142857142857142]\n",
      "['best.n.01', 'york.n.01', 0.06666666666666667]\n",
      "['best.n.01', 'investment.n.01', 0.1]\n",
      "['best.n.01', 'bank.n.01', 0.07692307692307693]\n",
      "['best.n.01', 'billion.n.01', 0.07692307692307693]\n",
      "['best.n.01', 'firm.n.01', 0.07692307692307693]\n",
      "['best.n.01', 'times.n.01', 0.09090909090909091]\n",
      "['best.n.01', 'million.n.01', 0.07692307692307693]\n",
      "['best.n.01', 'street.n.01', 0.0625]\n",
      "['best.n.01', 'company.n.01', 0.08333333333333333]\n",
      "['best.n.01', 'banks.n.01', 0.06666666666666667]\n",
      "['best.n.01', 'wall.n.01', 0.06666666666666667]\n",
      "['best.n.01', 'help.n.01', 0.25]\n",
      "['best.n.01', 'subscription.n.01', 0.07142857142857142]\n",
      "['best.n.01', 'business.n.01', 0.08333333333333333]\n",
      "['best.n.01', 'crisis.n.01', 0.07692307692307693]\n",
      "['best.n.01', 'fund.n.01', 0.07692307692307693]\n",
      "['best.n.01', 'trading.n.01', 0.125]\n",
      "['best.n.01', 'former.n.01', 0.07692307692307693]\n",
      "['best.n.01', 'deal.n.01', 0.14285714285714285]\n",
      "['best.n.01', 'market.n.01', 0.25]\n",
      "['best.n.01', 'abacus.n.01', 0.0625]\n",
      "['best.n.01', 'sec.n.01', 0.1]\n",
      "['limited.n.01', 'goldman.n.01', 0.08333333333333333]\n",
      "['limited.n.01', 'york.n.01', 0.05555555555555555]\n",
      "['limited.n.01', 'investment.n.01', 0.05555555555555555]\n",
      "['limited.n.01', 'bank.n.01', 0.1]\n",
      "['limited.n.01', 'billion.n.01', 0.0625]\n",
      "['limited.n.01', 'firm.n.01', 0.0625]\n",
      "['limited.n.01', 'times.n.01', 0.07142857142857142]\n",
      "['limited.n.01', 'million.n.01', 0.0625]\n",
      "['limited.n.01', 'street.n.01', 0.1111111111111111]\n",
      "['limited.n.01', 'company.n.01', 0.06666666666666667]\n",
      "['limited.n.01', 'banks.n.01', 0.07692307692307693]\n",
      "['limited.n.01', 'wall.n.01', 0.125]\n",
      "['limited.n.01', 'help.n.01', 0.06666666666666667]\n",
      "['limited.n.01', 'subscription.n.01', 0.058823529411764705]\n",
      "['limited.n.01', 'business.n.01', 0.06666666666666667]\n",
      "['limited.n.01', 'crisis.n.01', 0.0625]\n",
      "['limited.n.01', 'fund.n.01', 0.0625]\n",
      "['limited.n.01', 'trading.n.01', 0.0625]\n",
      "['limited.n.01', 'former.n.01', 0.0625]\n",
      "['limited.n.01', 'deal.n.01', 0.06666666666666667]\n",
      "['limited.n.01', 'market.n.01', 0.06666666666666667]\n",
      "['limited.n.01', 'abacus.n.01', 0.1111111111111111]\n",
      "['limited.n.01', 'sec.n.01', 0.07692307692307693]\n",
      "['india.n.01', 'goldman.n.01', 0.07142857142857142]\n",
      "['india.n.01', 'york.n.01', 0.05263157894736842]\n",
      "['india.n.01', 'investment.n.01', 0.05263157894736842]\n",
      "['india.n.01', 'bank.n.01', 0.09090909090909091]\n",
      "['india.n.01', 'billion.n.01', 0.058823529411764705]\n",
      "['india.n.01', 'firm.n.01', 0.058823529411764705]\n",
      "['india.n.01', 'times.n.01', 0.06666666666666667]\n",
      "['india.n.01', 'million.n.01', 0.058823529411764705]\n",
      "['india.n.01', 'street.n.01', 0.07142857142857142]\n",
      "['india.n.01', 'company.n.01', 0.0625]\n",
      "['india.n.01', 'banks.n.01', 0.06666666666666667]\n",
      "['india.n.01', 'wall.n.01', 0.07692307692307693]\n",
      "['india.n.01', 'help.n.01', 0.0625]\n",
      "['india.n.01', 'subscription.n.01', 0.05555555555555555]\n",
      "['india.n.01', 'business.n.01', 0.0625]\n",
      "['india.n.01', 'crisis.n.01', 0.058823529411764705]\n",
      "['india.n.01', 'fund.n.01', 0.058823529411764705]\n",
      "['india.n.01', 'trading.n.01', 0.058823529411764705]\n",
      "['india.n.01', 'former.n.01', 0.058823529411764705]\n",
      "['india.n.01', 'deal.n.01', 0.0625]\n",
      "['india.n.01', 'market.n.01', 0.0625]\n",
      "['india.n.01', 'abacus.n.01', 0.07142857142857142]\n",
      "['india.n.01', 'sec.n.01', 0.07142857142857142]\n",
      "['indonesia.n.01', 'goldman.n.01', 0.07692307692307693]\n",
      "['indonesia.n.01', 'york.n.01', 0.05555555555555555]\n",
      "['indonesia.n.01', 'investment.n.01', 0.05555555555555555]\n",
      "['indonesia.n.01', 'bank.n.01', 0.1]\n",
      "['indonesia.n.01', 'billion.n.01', 0.0625]\n",
      "['indonesia.n.01', 'firm.n.01', 0.0625]\n",
      "['indonesia.n.01', 'times.n.01', 0.07142857142857142]\n",
      "['indonesia.n.01', 'million.n.01', 0.0625]\n",
      "['indonesia.n.01', 'street.n.01', 0.07692307692307693]\n",
      "['indonesia.n.01', 'company.n.01', 0.06666666666666667]\n",
      "['indonesia.n.01', 'banks.n.01', 0.07142857142857142]\n",
      "['indonesia.n.01', 'wall.n.01', 0.08333333333333333]\n",
      "['indonesia.n.01', 'help.n.01', 0.06666666666666667]\n",
      "['indonesia.n.01', 'subscription.n.01', 0.058823529411764705]\n",
      "['indonesia.n.01', 'business.n.01', 0.06666666666666667]\n",
      "['indonesia.n.01', 'crisis.n.01', 0.0625]\n",
      "['indonesia.n.01', 'fund.n.01', 0.0625]\n",
      "['indonesia.n.01', 'trading.n.01', 0.0625]\n",
      "['indonesia.n.01', 'former.n.01', 0.0625]\n",
      "['indonesia.n.01', 'deal.n.01', 0.06666666666666667]\n",
      "['indonesia.n.01', 'market.n.01', 0.06666666666666667]\n",
      "['indonesia.n.01', 'abacus.n.01', 0.07692307692307693]\n",
      "['indonesia.n.01', 'sec.n.01', 0.07692307692307693]\n",
      "['taiwan.n.01', 'goldman.n.01', 0.1]\n",
      "['taiwan.n.01', 'york.n.01', 0.06666666666666667]\n",
      "['taiwan.n.01', 'investment.n.01', 0.06666666666666667]\n",
      "['taiwan.n.01', 'bank.n.01', 0.14285714285714285]\n",
      "['taiwan.n.01', 'billion.n.01', 0.07692307692307693]\n",
      "['taiwan.n.01', 'firm.n.01', 0.07692307692307693]\n",
      "['taiwan.n.01', 'times.n.01', 0.09090909090909091]\n",
      "['taiwan.n.01', 'million.n.01', 0.07692307692307693]\n",
      "['taiwan.n.01', 'street.n.01', 0.1]\n",
      "['taiwan.n.01', 'company.n.01', 0.08333333333333333]\n",
      "['taiwan.n.01', 'banks.n.01', 0.09090909090909091]\n",
      "['taiwan.n.01', 'wall.n.01', 0.1111111111111111]\n",
      "['taiwan.n.01', 'help.n.01', 0.08333333333333333]\n",
      "['taiwan.n.01', 'subscription.n.01', 0.07142857142857142]\n",
      "['taiwan.n.01', 'business.n.01', 0.08333333333333333]\n",
      "['taiwan.n.01', 'crisis.n.01', 0.07692307692307693]\n",
      "['taiwan.n.01', 'fund.n.01', 0.07692307692307693]\n",
      "['taiwan.n.01', 'trading.n.01', 0.07692307692307693]\n",
      "['taiwan.n.01', 'former.n.01', 0.07692307692307693]\n",
      "['taiwan.n.01', 'deal.n.01', 0.08333333333333333]\n",
      "['taiwan.n.01', 'market.n.01', 0.08333333333333333]\n",
      "['taiwan.n.01', 'abacus.n.01', 0.1]\n",
      "['taiwan.n.01', 'sec.n.01', 0.1]\n",
      "['corporation.n.01', 'goldman.n.01', 0.06666666666666667]\n",
      "['corporation.n.01', 'york.n.01', 0.08333333333333333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['corporation.n.01', 'investment.n.01', 0.0625]\n",
      "['corporation.n.01', 'bank.n.01', 0.07142857142857142]\n",
      "['corporation.n.01', 'billion.n.01', 0.07142857142857142]\n",
      "['corporation.n.01', 'firm.n.01', 0.5]\n",
      "['corporation.n.01', 'times.n.01', 0.08333333333333333]\n",
      "['corporation.n.01', 'million.n.01', 0.07142857142857142]\n",
      "['corporation.n.01', 'street.n.01', 0.058823529411764705]\n",
      "['corporation.n.01', 'company.n.01', 0.14285714285714285]\n",
      "['corporation.n.01', 'banks.n.01', 0.0625]\n",
      "['corporation.n.01', 'wall.n.01', 0.0625]\n",
      "['corporation.n.01', 'help.n.01', 0.07692307692307693]\n",
      "['corporation.n.01', 'subscription.n.01', 0.06666666666666667]\n",
      "['corporation.n.01', 'business.n.01', 0.3333333333333333]\n",
      "['corporation.n.01', 'crisis.n.01', 0.07142857142857142]\n",
      "['corporation.n.01', 'fund.n.01', 0.07142857142857142]\n",
      "['corporation.n.01', 'trading.n.01', 0.07142857142857142]\n",
      "['corporation.n.01', 'former.n.01', 0.07142857142857142]\n",
      "['corporation.n.01', 'deal.n.01', 0.07692307692307693]\n",
      "['corporation.n.01', 'market.n.01', 0.07692307692307693]\n",
      "['corporation.n.01', 'abacus.n.01', 0.058823529411764705]\n",
      "['corporation.n.01', 'sec.n.01', 0.09090909090909091]\n",
      "['government.n.01', 'goldman.n.01', 0.07692307692307693]\n",
      "['government.n.01', 'york.n.01', 0.1]\n",
      "['government.n.01', 'investment.n.01', 0.07142857142857142]\n",
      "['government.n.01', 'bank.n.01', 0.08333333333333333]\n",
      "['government.n.01', 'billion.n.01', 0.08333333333333333]\n",
      "['government.n.01', 'firm.n.01', 0.16666666666666666]\n",
      "['government.n.01', 'times.n.01', 0.1]\n",
      "['government.n.01', 'million.n.01', 0.08333333333333333]\n",
      "['government.n.01', 'street.n.01', 0.06666666666666667]\n",
      "['government.n.01', 'company.n.01', 0.2]\n",
      "['government.n.01', 'banks.n.01', 0.07142857142857142]\n",
      "['government.n.01', 'wall.n.01', 0.07142857142857142]\n",
      "['government.n.01', 'help.n.01', 0.09090909090909091]\n",
      "['government.n.01', 'subscription.n.01', 0.07692307692307693]\n",
      "['government.n.01', 'business.n.01', 0.2]\n",
      "['government.n.01', 'crisis.n.01', 0.08333333333333333]\n",
      "['government.n.01', 'fund.n.01', 0.08333333333333333]\n",
      "['government.n.01', 'trading.n.01', 0.08333333333333333]\n",
      "['government.n.01', 'former.n.01', 0.08333333333333333]\n",
      "['government.n.01', 'deal.n.01', 0.09090909090909091]\n",
      "['government.n.01', 'market.n.01', 0.09090909090909091]\n",
      "['government.n.01', 'abacus.n.01', 0.06666666666666667]\n",
      "['government.n.01', 'sec.n.01', 0.1111111111111111]\n",
      "['billion.n.01', 'goldman.n.01', 0.07142857142857142]\n",
      "['billion.n.01', 'york.n.01', 0.06666666666666667]\n",
      "['billion.n.01', 'investment.n.01', 0.06666666666666667]\n",
      "['billion.n.01', 'bank.n.01', 0.07692307692307693]\n",
      "['billion.n.01', 'billion.n.01', 1.0]\n",
      "['billion.n.01', 'firm.n.01', 0.07692307692307693]\n",
      "['billion.n.01', 'times.n.01', 0.1111111111111111]\n",
      "['billion.n.01', 'million.n.01', 0.3333333333333333]\n",
      "['billion.n.01', 'street.n.01', 0.0625]\n",
      "['billion.n.01', 'company.n.01', 0.08333333333333333]\n",
      "['billion.n.01', 'banks.n.01', 0.06666666666666667]\n",
      "['billion.n.01', 'wall.n.01', 0.06666666666666667]\n",
      "['billion.n.01', 'help.n.01', 0.08333333333333333]\n",
      "['billion.n.01', 'subscription.n.01', 0.07142857142857142]\n",
      "['billion.n.01', 'business.n.01', 0.08333333333333333]\n",
      "['billion.n.01', 'crisis.n.01', 0.07692307692307693]\n",
      "['billion.n.01', 'fund.n.01', 0.09090909090909091]\n",
      "['billion.n.01', 'trading.n.01', 0.07692307692307693]\n",
      "['billion.n.01', 'former.n.01', 0.07692307692307693]\n",
      "['billion.n.01', 'deal.n.01', 0.08333333333333333]\n",
      "['billion.n.01', 'market.n.01', 0.08333333333333333]\n",
      "['billion.n.01', 'abacus.n.01', 0.0625]\n",
      "['billion.n.01', 'sec.n.01', 0.125]\n",
      "['&amp.n.01', '&amp.n.01', 1]\n",
      "['finance.n.01', 'goldman.n.01', 0.06666666666666667]\n",
      "['finance.n.01', 'york.n.01', 0.0625]\n",
      "['finance.n.01', 'investment.n.01', 0.5]\n",
      "['finance.n.01', 'bank.n.01', 0.07142857142857142]\n",
      "['finance.n.01', 'billion.n.01', 0.07142857142857142]\n",
      "['finance.n.01', 'firm.n.01', 0.07142857142857142]\n",
      "['finance.n.01', 'times.n.01', 0.08333333333333333]\n",
      "['finance.n.01', 'million.n.01', 0.07142857142857142]\n",
      "['finance.n.01', 'street.n.01', 0.058823529411764705]\n",
      "['finance.n.01', 'company.n.01', 0.07692307692307693]\n",
      "['finance.n.01', 'banks.n.01', 0.0625]\n",
      "['finance.n.01', 'wall.n.01', 0.0625]\n",
      "['finance.n.01', 'help.n.01', 0.125]\n",
      "['finance.n.01', 'subscription.n.01', 0.06666666666666667]\n",
      "['finance.n.01', 'business.n.01', 0.07692307692307693]\n",
      "['finance.n.01', 'crisis.n.01', 0.07142857142857142]\n",
      "['finance.n.01', 'fund.n.01', 0.07142857142857142]\n",
      "['finance.n.01', 'trading.n.01', 0.25]\n",
      "['finance.n.01', 'former.n.01', 0.07142857142857142]\n",
      "['finance.n.01', 'deal.n.01', 0.2]\n",
      "['finance.n.01', 'market.n.01', 0.125]\n",
      "['finance.n.01', 'abacus.n.01', 0.058823529411764705]\n",
      "['finance.n.01', 'sec.n.01', 0.09090909090909091]\n",
      "['programme.n.01', 'goldman.n.01', 0.07692307692307693]\n",
      "['programme.n.01', 'york.n.01', 0.07142857142857142]\n",
      "['programme.n.01', 'investment.n.01', 0.07142857142857142]\n",
      "['programme.n.01', 'bank.n.01', 0.08333333333333333]\n",
      "['programme.n.01', 'billion.n.01', 0.08333333333333333]\n",
      "['programme.n.01', 'firm.n.01', 0.08333333333333333]\n",
      "['programme.n.01', 'times.n.01', 0.1]\n",
      "['programme.n.01', 'million.n.01', 0.08333333333333333]\n",
      "['programme.n.01', 'street.n.01', 0.06666666666666667]\n",
      "['programme.n.01', 'company.n.01', 0.09090909090909091]\n",
      "['programme.n.01', 'banks.n.01', 0.07142857142857142]\n",
      "['programme.n.01', 'wall.n.01', 0.07142857142857142]\n",
      "['programme.n.01', 'help.n.01', 0.09090909090909091]\n",
      "['programme.n.01', 'subscription.n.01', 0.07692307692307693]\n",
      "['programme.n.01', 'business.n.01', 0.09090909090909091]\n",
      "['programme.n.01', 'crisis.n.01', 0.08333333333333333]\n",
      "['programme.n.01', 'fund.n.01', 0.08333333333333333]\n",
      "['programme.n.01', 'trading.n.01', 0.08333333333333333]\n",
      "['programme.n.01', 'former.n.01', 0.08333333333333333]\n",
      "['programme.n.01', 'deal.n.01', 0.09090909090909091]\n",
      "['programme.n.01', 'market.n.01', 0.09090909090909091]\n",
      "['programme.n.01', 'abacus.n.01', 0.06666666666666667]\n",
      "['programme.n.01', 'sec.n.01', 0.1111111111111111]\n",
      "['device.n.01', 'goldman.n.01', 0.1]\n",
      "['device.n.01', 'york.n.01', 0.0625]\n",
      "['device.n.01', 'investment.n.01', 0.0625]\n",
      "['device.n.01', 'bank.n.01', 0.125]\n",
      "['device.n.01', 'billion.n.01', 0.07142857142857142]\n",
      "['device.n.01', 'firm.n.01', 0.07142857142857142]\n",
      "['device.n.01', 'times.n.01', 0.08333333333333333]\n",
      "['device.n.01', 'million.n.01', 0.07142857142857142]\n",
      "['device.n.01', 'street.n.01', 0.14285714285714285]\n",
      "['device.n.01', 'company.n.01', 0.07692307692307693]\n",
      "['device.n.01', 'banks.n.01', 0.09090909090909091]\n",
      "['device.n.01', 'wall.n.01', 0.16666666666666666]\n",
      "['device.n.01', 'help.n.01', 0.07692307692307693]\n",
      "['device.n.01', 'subscription.n.01', 0.06666666666666667]\n",
      "['device.n.01', 'business.n.01', 0.07692307692307693]\n",
      "['device.n.01', 'crisis.n.01', 0.07142857142857142]\n",
      "['device.n.01', 'fund.n.01', 0.07142857142857142]\n",
      "['device.n.01', 'trading.n.01', 0.07142857142857142]\n",
      "['device.n.01', 'former.n.01', 0.07142857142857142]\n",
      "['device.n.01', 'deal.n.01', 0.07692307692307693]\n",
      "['device.n.01', 'market.n.01', 0.07692307692307693]\n",
      "['device.n.01', 'abacus.n.01', 0.14285714285714285]\n",
      "['device.n.01', 'sec.n.01', 0.09090909090909091]\n",
      "['hotspot.n.01', 'goldman.n.01', 0.09090909090909091]\n",
      "['hotspot.n.01', 'york.n.01', 0.0625]\n",
      "['hotspot.n.01', 'investment.n.01', 0.0625]\n",
      "['hotspot.n.01', 'bank.n.01', 0.125]\n",
      "['hotspot.n.01', 'billion.n.01', 0.07142857142857142]\n",
      "['hotspot.n.01', 'firm.n.01', 0.07142857142857142]\n",
      "['hotspot.n.01', 'times.n.01', 0.08333333333333333]\n",
      "['hotspot.n.01', 'million.n.01', 0.07142857142857142]\n",
      "['hotspot.n.01', 'street.n.01', 0.09090909090909091]\n",
      "['hotspot.n.01', 'company.n.01', 0.07692307692307693]\n",
      "['hotspot.n.01', 'banks.n.01', 0.08333333333333333]\n",
      "['hotspot.n.01', 'wall.n.01', 0.1]\n",
      "['hotspot.n.01', 'help.n.01', 0.07692307692307693]\n",
      "['hotspot.n.01', 'subscription.n.01', 0.06666666666666667]\n",
      "['hotspot.n.01', 'business.n.01', 0.07692307692307693]\n",
      "['hotspot.n.01', 'crisis.n.01', 0.07142857142857142]\n",
      "['hotspot.n.01', 'fund.n.01', 0.07142857142857142]\n",
      "['hotspot.n.01', 'trading.n.01', 0.07142857142857142]\n",
      "['hotspot.n.01', 'former.n.01', 0.07142857142857142]\n",
      "['hotspot.n.01', 'deal.n.01', 0.07692307692307693]\n",
      "['hotspot.n.01', 'market.n.01', 0.07692307692307693]\n",
      "['hotspot.n.01', 'abacus.n.01', 0.09090909090909091]\n",
      "['hotspot.n.01', 'sec.n.01', 0.09090909090909091]\n",
      "Related index is 35%\n"
     ]
    }
   ],
   "source": [
    "#import nltk\n",
    "#nltk.download('wordnet')\n",
    "#from nltk.corpus import wordnet\n",
    "#wn = nltk.corpus.wordnet #the corpus reader\n",
    "\n",
    "dotn01 = ('.','n','.','0','1')\n",
    "dbslist = []\n",
    "for key in dbs.commonwords(40):\n",
    "    dbsslice = list(key)\n",
    "    for letter in dotn01:\n",
    "        dbsslice.append(letter)\n",
    "    dbsslice = ''.join(dbsslice)\n",
    "    dbslist.append(dbsslice)\n",
    "\n",
    "print(dbslist)\n",
    "\n",
    "goldmanlist = []\n",
    "for key in goldman.commonwords(40):\n",
    "    goldmanslice = list(key)\n",
    "    for letter in dotn01:\n",
    "        goldmanslice.append(letter)\n",
    "    goldmanslice = ''.join(goldmanslice)\n",
    "    goldmanlist.append(goldmanslice)\n",
    "    \n",
    "print(goldmanlist)\n",
    "    \n",
    "count = 0\n",
    "sum = 0\n",
    "\n",
    "for word1 in dbslist:\n",
    "    for word2 in goldmanlist:\n",
    "        try:\n",
    "            x = wn.synset(word1)\n",
    "            y = wn.synset(word2)\n",
    "            sum += x.path_similarity(y) * math.exp(2.5 * x.path_similarity(y)) + 14 * math.log(0.92+x.path_similarity(y))\n",
    "            count += 1\n",
    "            print([word1,word2,x.path_similarity(y)])\n",
    "        except:\n",
    "            if word1 == word2 and re.findall(r\"\\d+.n.01\", word1) == []: #remove numbers being counted as match yyyy.n.01\n",
    "                sum += 21.32\n",
    "                count += 1\n",
    "                print([word1,word2,1])\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "if count != 0:\n",
    "    percent = round(sum/count*100)\n",
    "    print('Related index is ' + str(percent) + '%')\n",
    "else:\n",
    "    print('No relation index can be calculated as words are all foreign')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bank.n.01', 'dbs.n.01', 'singapore.n.01', 'banking.n.01', 'banks.n.01', 'holdings.n.01', 'china.n.01', 'commercial.n.01', 'development.n.01', 'temasek.n.01', 'trust.n.01', 'branches.n.01', 'asia.n.01', 'posb.n.01', 'financial.n.01', 'best.n.01', 'customers.n.01', 'limited.n.01', '1968.n.01', 'industrial.n.01', 'shares.n.01', '2016.n.01', 'united.n.01', 'india.n.01', 'indonesia.n.01', 'taiwan.n.01', 'mbanking.n.01', 'corporation.n.01', 'government.n.01', 'billion.n.01', '2017.n.01', 'hong.n.01', '&amp.n.01', 'global.n.01', '2015.n.01', 'finance.n.01', 'programme.n.01', 'device.n.01', 'hotspot.n.01', 'startups.n.01']\n",
      "['goldman.n.01', 'sachs.n.01', 'york.n.01', 'securities.n.01', 'investment.n.01', 'bank.n.01', 'billion.n.01', '2010.n.01', '2013.n.01', 'firm.n.01', 'times.n.01', 'million.n.01', 'street.n.01', 'were.n.01', 'financial.n.01', 'had.n.01', '2008.n.01', '2011.n.01', 'company.n.01', '2009.n.01', 'banks.n.01', 'wall.n.01', 'help.n.01', 'subscription.n.01', 'required.n.01', 'business.n.01', 'goldmans.n.01', 'crisis.n.01', 'fund.n.01', '&amp.n.01', 'trading.n.01', 'former.n.01', 'employees.n.01', 'deal.n.01', 'investors.n.01', '2014.n.01', 'market.n.01', 'bloomberg.n.01', 'abacus.n.01', 'sec.n.01']\n",
      "Related index is 38%\n"
     ]
    }
   ],
   "source": [
    "#import nltk\n",
    "#nltk.download('wordnet')\n",
    "#from nltk.corpus import wordnet\n",
    "#wn = nltk.corpus.wordnet #the corpus reader\n",
    "\n",
    "dotn01 = ('.','n','.','0','1')\n",
    "dbslist = []\n",
    "for key in dbs.commonwords(40):\n",
    "    dbsslice = list(key)\n",
    "    for letter in dotn01:\n",
    "        dbsslice.append(letter)\n",
    "    dbsslice = ''.join(dbsslice)\n",
    "    dbslist.append(dbsslice)\n",
    "\n",
    "print(dbslist)\n",
    "\n",
    "goldmanlist = []\n",
    "for key in goldman.commonwords(40):\n",
    "    goldmanslice = list(key)\n",
    "    for letter in dotn01:\n",
    "        goldmanslice.append(letter)\n",
    "    goldmanslice = ''.join(goldmanslice)\n",
    "    goldmanlist.append(goldmanslice)\n",
    "    \n",
    "print(goldmanlist)\n",
    "    \n",
    "count = 0\n",
    "sum = 0\n",
    "\n",
    "for word1 in dbslist:\n",
    "    for word2 in goldmanlist:\n",
    "        try:\n",
    "            x = wn.synset(word1)\n",
    "            y = wn.synset(word2)\n",
    "            sum += x.path_similarity(y) * math.exp(3 * x.path_similarity(y)) + 7 * math.log(0.92+x.path_similarity(y))\n",
    "            count += 1\n",
    "        except:\n",
    "            if word1 == word2 and re.findall(r\"\\d+.n.01\", word1) == []: #remove numbers being counted as match yyyy.n.01\n",
    "                sum += 24.65\n",
    "                count += 1\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "if count != 0:\n",
    "    percent = round(sum/count*100)\n",
    "    print('Related index is ' + str(percent) + '%')\n",
    "else:\n",
    "    print('No relation index can be calculated as words are all foreign')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "word3 = '1a.n.01'\n",
    "if re.findall(r\"\\d+.n.01\", word3) == []:\n",
    "    print (True)\n",
    "else:\n",
    "    print(False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
